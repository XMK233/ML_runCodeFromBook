{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998e716b-8481-45d2-9348-90c49ec22ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb91a42-3dfa-4b54-ab09-a6e268eefda7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51bfb12e-7eb5-4dd1-87ae-ea83568dae38",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a49f853f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.path.append('..')\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f5dccba",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(15, dtype=torch.float64).view(5, 3)\n",
    "## 相当于生成0～15的数组，然后扭成5*3的矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3134ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.],\n",
       "        [ 3.,  4.,  5.],\n",
       "        [ 6.,  7.,  8.],\n",
       "        [ 9., 10., 11.],\n",
       "        [12., 13., 14.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f40b2913",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = torch.ones(x.shape[1]) ## 方差？\n",
    "beta = torch.zeros(x.shape[1]) ## 均值？\n",
    "## 跟列的形状保持一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acf22219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db51d4a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3c0a32c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6., 7., 8.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_mean = torch.mean(x, dim=0, keepdim=True)\n",
    "## 如果我们直接照搬原来的代码，是跑不了的。\n",
    "## 这边求平均的时候，说，只支持数据格式为float。\n",
    "## 呵呵哒。\n",
    "## 这里就是在行这个维度求平均。或者说，取一列出来，求平均。\n",
    "## 呵呵哒。\n",
    "x_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "357b96c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-6., -6., -6.],\n",
       "        [-3., -3., -3.],\n",
       "        [ 0.,  0.,  0.],\n",
       "        [ 3.,  3.,  3.],\n",
       "        [ 6.,  6.,  6.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x - x_mean\n",
    "## 这里的减法操作，相当于每一行各自相减。\n",
    "## 这个操作是我没有想到的。\n",
    "## 两个相减的张量，形状都不一样，怎么会能相减呢？\n",
    "## 这样看来，还真是出乎我的意料啊。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42999937",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_var = torch.mean((x - x_mean)**2, dim=0, keepdim=True)\n",
    "## 这里的平方，也是每一个元素各自平方。\n",
    "## 所以如果要矩阵相乘，可能得用torch.mm之类的方法？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "882e5764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[18., 18., 18.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fa0bf1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4142, -1.4142, -1.4142],\n",
       "        [-0.7071, -0.7071, -0.7071],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.7071,  0.7071,  0.7071],\n",
       "        [ 1.4142,  1.4142,  1.4142]], dtype=torch.float64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eps = 1e-5\n",
    "\n",
    "x_hat = (x - x_mean) / torch.sqrt(x_var + eps)\n",
    "\n",
    "x_hat\n",
    "## 这个就是批归一化的结果了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "654e5783",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 所以上述的操作，可以理解成：\n",
    "## batch中，在某个维度上的所有值进行normalization。然后各个维度独立开展这样的操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2f17fe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6., 7., 8.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd1ad9ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9645c956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma.view_as(x_mean) \n",
    "## 看起来就是将 gamma 改变成 x_mean 的形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a198c65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4142, -1.4142, -1.4142],\n",
       "        [-0.7071, -0.7071, -0.7071],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.7071,  0.7071,  0.7071],\n",
       "        [ 1.4142,  1.4142,  1.4142]], dtype=torch.float64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aad3d1a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.8284, -2.8284, -2.8284],\n",
       "        [-1.4142, -1.4142, -1.4142],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 1.4142,  1.4142,  1.4142],\n",
       "        [ 2.8284,  2.8284,  2.8284]], dtype=torch.float64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(gamma.view_as(x_mean)*2) * x_hat \\ ## 这里我感觉好像是在说ho，gamma原来是一维的，现在改成二维的。然后跟x_hat对应的维度的，进行对应位置相乘。\n",
    "+ beta.view_as(x_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "520131aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision.datasets import mnist # 导入 pytorch 内置的 mnist 数据\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb752796",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.ReLU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0024d69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
