{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1da7f5e-294f-4007-97b1-f6e958c41b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "storage dir: D:\\forCoding_data\\ML_runCodeFromBook\\深入浅出图神经网络\n",
      "code dir: D:\\forCoding_code\\ML_runCodeFromBook\\深入浅出图神经网络\n"
     ]
    }
   ],
   "source": [
    "import random, os, tqdm, time, json, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "import sys\n",
    "random.seed(618)\n",
    "np.random.seed(907)\n",
    "\n",
    "# sys.path.append(\"../../../\")\n",
    "# new_base_path = os.path.join(\n",
    "#     \"/Users/minkexiu/Downloads/\",\n",
    "#     \"/\".join(\n",
    "#         os.getcwd().split(\"/\")[-1*(len(sys.path[-1].split(\"/\")) - 1):]\n",
    "#     ),\n",
    "# )\n",
    "\n",
    "sys.path.append(\"..\\\\..\\\\\")\n",
    "new_base_path = os.path.join(\n",
    "    \"D:\\\\forCoding_data\\\\\",\n",
    "    \"\\\\\".join(\n",
    "        os.getcwd().split(\"\\\\\")[-1*(len(sys.path[-1].split(\"\\\\\")) - 1):]\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"storage dir:\", new_base_path)\n",
    "print(\"code dir:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11b46c75-b46e-4958-ab9d-0f10f247b5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 17 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:165: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:165: SyntaxWarning: invalid escape sequence '\\d'\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_19436\\2849389036.py:165: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  *[int(x) for x in re.findall(\"\\d+\", zh_date_str)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>天天</th>\n",
       "      <th>天天</th>\n",
       "      <th>风天</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>上卦</th>\n",
       "      <td>☰乾金</td>\n",
       "      <td>☰乾金</td>\n",
       "      <td>☴巽木</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>下卦</th>\n",
       "      <td>☰乾金</td>\n",
       "      <td>☰乾金</td>\n",
       "      <td>☰乾金</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     天天   天天   风天\n",
       "上卦  ☰乾金  ☰乾金  ☴巽木\n",
       "下卦  ☰乾金  ☰乾金  ☰乾金"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08 15 10 酉时\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>地山</th>\n",
       "      <th>雷水</th>\n",
       "      <th>雷山</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>上卦</th>\n",
       "      <td>☷坤土</td>\n",
       "      <td>☳震木</td>\n",
       "      <td>☳震木</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>下卦</th>\n",
       "      <td>☶艮土</td>\n",
       "      <td>☵坎水</td>\n",
       "      <td>☶艮土</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     地山   雷水   雷山\n",
       "上卦  ☷坤土  ☳震木  ☳震木\n",
       "下卦  ☶艮土  ☵坎水  ☶艮土"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('001000', '010100', '001100')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 创建文件夹。\n",
    "if not os.path.exists(new_base_path):\n",
    "    os.makedirs(\n",
    "        new_base_path\n",
    "    )\n",
    "if not os.path.exists(os.path.join(new_base_path, \"preprocessedData\")):\n",
    "    os.makedirs(\n",
    "        os.path.join(new_base_path, \"preprocessedData\")\n",
    "    )\n",
    "if not os.path.exists(os.path.join(new_base_path, \"originalData\")):\n",
    "    os.makedirs(\n",
    "        os.path.join(new_base_path, \"originalData\")\n",
    "    )\n",
    "if not os.path.exists(os.path.join(new_base_path, \"trained_models\")):\n",
    "    os.makedirs(\n",
    "        os.path.join(new_base_path, \"trained_models\")\n",
    "    )\n",
    "\n",
    "def create_originalData_path(filename_or_path):\n",
    "    return os.path.join(new_base_path, \"originalData\", filename_or_path)\n",
    "def create_preprocessedData_path(filename_or_path):\n",
    "    return os.path.join(new_base_path, \"preprocessedData\", filename_or_path)\n",
    "def create_trained_models_path(filename_or_path):\n",
    "    return os.path.join(new_base_path, \"trained_models\", filename_or_path)\n",
    "\n",
    "def millisec2datetime(timestamp):\n",
    "    time_local = time.localtime(timestamp/1000)\n",
    "    return time.strftime(\"%Y-%m-%d %H:%M:%S\", time_local)\n",
    "    \n",
    "def run_finish():\n",
    "    # 假设你的字体文件是 'myfont.ttf' 并且位于当前目录下  \n",
    "    font = FontProperties(fname=\"/Users/minkexiu/Documents/GitHub/ML_Tryout/SimHei.ttf\", size=24)  \n",
    "    # 创建一个空白的图形  \n",
    "    fig, ax = plt.subplots()  \n",
    "    ax.imshow(\n",
    "        plt.imread(\"/Users/minkexiu/Downloads/wallhaven-dgxpyg.jpg\")\n",
    "    )\n",
    "    # 在图形中添加文字  \n",
    "    ax.text(\n",
    "        ax.get_xlim()[1] * 0.5, \n",
    "        ax.get_ylim()[0] * 0.5, \n",
    "        f\"程序于这个点跑完：\\n{millisec2datetime(time.time()*1000)}\", fontproperties=font, ha=\"center\", va=\"center\", color=\"red\"\n",
    "    )  \n",
    "    # 设置图形的布局  \n",
    "    # ax.set_xlim(0, 1)  \n",
    "    # ax.set_ylim(0, 1)  \n",
    "    ax.set_xticks([])  \n",
    "    ax.set_yticks([])  \n",
    "    ax.patch.set_color(\"blue\")\n",
    "    # 显示图形  \n",
    "    plt.show()\n",
    "        \n",
    "tqdm.tqdm.pandas() ## 引入这个，就可以在apply的时候用progress_apply了。\n",
    "\n",
    "import IPython\n",
    "def kill_current_kernel():\n",
    "    '''杀死当前的kernel释放内存空间。'''\n",
    "    IPython.Application.instance().kernel.do_shutdown(True) \n",
    "    \n",
    "def simply_show_data(df1):\n",
    "    print(df1.shape)\n",
    "    display(df1.head())\n",
    "    \n",
    "def wait_flag(saved_flag_path, time_interval_sec=10):\n",
    "    print(\"waiting for\", saved_flag_path)\n",
    "    time_count = 0\n",
    "    while True:\n",
    "        if os.path.exists(saved_flag_path):\n",
    "            break\n",
    "        time.sleep(time_interval_sec)\n",
    "        time_count+=time_interval_sec\n",
    "        print(time_count, end=\" \")\n",
    "    print(\"finish!!\")\n",
    "\n",
    "def parallelly_run_multiple_similar_python_code(codes, nb_workers = 4):\n",
    "    '''\n",
    "    codes是多条相似的python代码。\n",
    "    这个函数的作用就是将其平行地跑，每一条python代码就对应一个线程。或许可以后续优化，比如固定线程数为一个特定值。\n",
    "    nb_workers 如果赋值为\n",
    "    '''\n",
    "    assert (isinstance(nb_workers, int)), \"`nb_workers' should be int.\"\n",
    "    df_sqls = pd.DataFrame(\n",
    "        {\n",
    "            \"func\": codes\n",
    "\n",
    "        }\n",
    "    )\n",
    "    display(df_sqls)\n",
    "    from pandarallel import pandarallel\n",
    "    pandarallel.initialize(nb_workers = df_sqls.shape[0] if nb_workers<0 else nb_workers, progress_bar = True)\n",
    "    def run_sql_prlly(row):\n",
    "        try: \n",
    "            cmd = f'{row[\"func\"]}'\n",
    "            print(cmd, \"\\n\")\n",
    "            eval(cmd)\n",
    "            return \"0-success\"\n",
    "        except Exception as e:\n",
    "            return e\n",
    "    df_sqls[\"run_rsts\"] = df_sqls.parallel_apply(lambda row: run_sql_prlly(row), axis = 1)\n",
    "    display(df_sqls)\n",
    "    \n",
    "def create_originalData_path(filename_or_path):\n",
    "    return os.path.join(new_base_path, \"originalData\", filename_or_path)\n",
    "def create_preprocessedData_path(filename_or_path):\n",
    "    return os.path.join(new_base_path, \"preprocessedData\", filename_or_path)\n",
    "def create_trained_models_path(filename_or_path):\n",
    "    return os.path.join(new_base_path, \"trained_models\", filename_or_path)\n",
    "    \n",
    "class TimerContext:  \n",
    "    def __enter__(self):  \n",
    "        self.start_time = str(datetime.now())\n",
    "        print(\"start time:\", self.start_time)\n",
    "        return self  \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):  \n",
    "        print(\"start time:\", self.start_time)\n",
    "        print(\"end time\", str(datetime.now()))\n",
    "\n",
    "def three_num_get_gua(a, b, c):\n",
    "    '''梅花易数三数起卦，以取本、互、变。'''\n",
    "    bagua = [\"111\", \"110\", \"101\", \"100\", \"011\", \"010\", \"001\", \"000\"]\n",
    "    guatu = {\n",
    "        \"111\": (\"☰\", \"天\", \"乾金\"), \n",
    "        \"110\": (\"☱\", \"泽\", \"兑金\"),\n",
    "        \"101\": (\"☲\", \"火\", \"离火\"),\n",
    "        \"100\": (\"☳\" , \"雷\", \"震木\"),\n",
    "        \"011\": (\"☴\", \"风\", \"巽木\"),\n",
    "        \"010\": (\"☵\", \"水\", \"坎水\"),\n",
    "        \"001\": (\"☶\", \"山\", \"艮土\"),\n",
    "        \"000\": (\"☷\", \"地\", \"坤土\"),\n",
    "    }\n",
    "    shanggua_idx = 7 if (a % 8 == 0) else (a % 8 - 1)\n",
    "    xiagua_idx = 7 if (b % 8 == 0) else (b % 8 - 1)\n",
    "    bianyao_idx = 5 if (c % 6 == 0) else (c % 6 - 1)\n",
    "    bengua = bagua[xiagua_idx] + bagua[shanggua_idx]\n",
    "    hugua = bengua[1:-1][:3] + bengua[1:-1][1:]\n",
    "    biangua = list(bengua)\n",
    "    biangua[bianyao_idx] = str(1 - int(biangua[bianyao_idx]))\n",
    "    biangua = \"\".join(biangua)\n",
    "    df = pd.DataFrame([[\n",
    "        guatu[bengua[3:]][0]+guatu[bengua[3:]][2], guatu[hugua[3:]][0]+guatu[hugua[3:]][2], guatu[biangua[3:]][0]+guatu[biangua[3:]][2], \n",
    "    ],[\n",
    "        guatu[bengua[:3]][0]+guatu[bengua[:3]][2], guatu[hugua[:3]][0]+guatu[hugua[:3]][2], guatu[biangua[:3]][0]+guatu[biangua[:3]][2], \n",
    "    ]], index=[\"上卦\", \"下卦\"], columns = [\n",
    "        guatu[bengua[3:]][1] + guatu[bengua[:3]][1],\n",
    "        guatu[hugua[3:]][1] + guatu[hugua[:3]][1],\n",
    "        guatu[biangua[3:]][1] + guatu[biangua[:3]][1],\n",
    "    ])\n",
    "    display(df)\n",
    "    return bengua, hugua, biangua\n",
    "    \n",
    "def easy_start_gua():\n",
    "    \"\"\"用公历的日、时、分来起卦。\"\"\"\n",
    "    n1, n2, n3 = str(datetime.now())[8:10], str(datetime.now())[11:13], str(datetime.now())[14:16]\n",
    "    print(n1, n2, n3)\n",
    "    return three_num_get_gua(int(n1), int(n2), int(n3))\n",
    "easy_start_gua()\n",
    "\n",
    "import zhdate\n",
    "def easy_start_gua_lunar():\n",
    "    '''用农历的月、日、时辰来起卦。'''\n",
    "    time_now = datetime.now()\n",
    "    zh_date_str = str(zhdate.ZhDate.from_datetime(time_now))\n",
    "    zh_date_str_1 = datetime.strftime(\n",
    "        datetime(\n",
    "            *[int(x) for x in re.findall(\"\\d+\", zh_date_str)]\n",
    "        ),\n",
    "        '%Y-%m-%d'\n",
    "    )\n",
    "    zh_hour = (time_now.hour + 1)//2%12+1\n",
    "    zh_hour_dizhi = \"子、丑、寅、卯、辰、巳、午、未、申、酉、戌、亥\".split(\"、\")[zh_hour-1]\n",
    "    \n",
    "    n1, n2, n3 = zh_date_str_1[5:7], zh_date_str_1[8:10], zh_hour\n",
    "    print(n1, n2, n3, f\"{zh_hour_dizhi}时\")\n",
    "    return three_num_get_gua(int(n1), int(n2), int(n3))\n",
    "easy_start_gua_lunar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8157223-fdf5-421f-8579-559cb7dbf393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# from net import GraphSage\n",
    "# from data import CoraData\n",
    "# from sampling import multihop_sampling\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import pickle\n",
    "import itertools\n",
    "import scipy.sparse as sp\n",
    "import urllib\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87919a16-b93a-49e7-a978-af164880f0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 装一下pytorch什么的库吧。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "836aaf04-4cd2-451b-894e-7dbb661eba63",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 定义数据：\n",
    "\n",
    "class CoraData(object):\n",
    "    download_url = \"https://github.com/kimiyoung/planetoid/raw/master/data\"\n",
    "    filenames = [\n",
    "        \"ind.cora.{}\".format(name) for name in\n",
    "        ['x', 'tx', 'allx', 'y', 'ty', 'ally', 'graph', 'test.index']\n",
    "    ]\n",
    "\n",
    "    def __init__(self, data_root=\"cora\", rebuild=False):\n",
    "        \"\"\"Cora数据，包括数据下载，处理，加载等功能\n",
    "        当数据的缓存文件存在时，将使用缓存文件，否则将下载、进行处理，并缓存到磁盘\n",
    "\n",
    "        处理之后的数据可以通过属性 .data 获得，它将返回一个数据对象，包括如下几部分：\n",
    "            * x: 节点的特征，维度为 2708 * 1433，类型为 np.ndarray\n",
    "            * y: 节点的标签，总共包括7个类别，类型为 np.ndarray\n",
    "            * adjacency: 邻接矩阵，维度为 2708 * 2708，类型为 scipy.sparse.coo.coo_matrix\n",
    "            * train_mask: 训练集掩码向量，维度为 2708，当节点属于训练集时，相应位置为True，否则False\n",
    "            * val_mask: 验证集掩码向量，维度为 2708，当节点属于验证集时，相应位置为True，否则False\n",
    "            * test_mask: 测试集掩码向量，维度为 2708，当节点属于测试集时，相应位置为True，否则False\n",
    "\n",
    "        Args:\n",
    "        -------\n",
    "            data_root: string, optional\n",
    "                存放数据的目录，原始数据路径: ../data/cora\n",
    "                缓存数据路径: {data_root}/ch5_cached.pkl\n",
    "            rebuild: boolean, optional\n",
    "                是否需要重新构建数据集，当设为True时，如果存在缓存数据也会重建数据\n",
    "\n",
    "        \"\"\"\n",
    "        print(\"init...\")\n",
    "        self.data_root = create_originalData_path(data_root)\n",
    "        save_file = osp.join(self.data_root, \"ch5_cached.pkl\")\n",
    "        if osp.exists(save_file) and not rebuild:\n",
    "            print(\"Using Cached file: {}\".format(save_file))\n",
    "            self._data = pickle.load(open(save_file, \"rb\"))\n",
    "        else:\n",
    "            if not osp.exists(self.data_root):\n",
    "                os.makedirs(self.data_root)\n",
    "            self._data = self.process_data()\n",
    "            with open(save_file, \"wb\") as f:\n",
    "                pickle.dump(self.data, f)\n",
    "            print(\"Cached file: {}\".format(save_file))\n",
    "    \n",
    "    @property\n",
    "    def data(self):\n",
    "        \"\"\"返回Data数据对象，包括x, y, adjacency, train_mask, val_mask, test_mask\"\"\"\n",
    "        return self._data\n",
    "        \n",
    "    def process_data(self):\n",
    "        \"\"\"\n",
    "        处理数据，得到节点特征和标签，邻接矩阵，训练集、验证集以及测试集\n",
    "        引用自：https://github.com/rusty1s/pytorch_geometric\n",
    "        \"\"\"\n",
    "        print(\"Process data ...\")\n",
    "        _, tx, allx, y, ty, ally, graph, test_index = [\n",
    "            self.read_data(\n",
    "                osp.join(self.data_root, name)\n",
    "            ) for name in self.filenames\n",
    "        ]\n",
    "        train_index = np.arange(y.shape[0])\n",
    "        val_index = np.arange(y.shape[0], y.shape[0] + 500)\n",
    "        sorted_test_index = sorted(test_index)\n",
    "\n",
    "        x = np.concatenate((allx, tx), axis=0)\n",
    "        y = np.concatenate((ally, ty), axis=0).argmax(axis=1)\n",
    "\n",
    "        x[test_index] = x[sorted_test_index]\n",
    "        y[test_index] = y[sorted_test_index]\n",
    "        num_nodes = x.shape[0]\n",
    "\n",
    "        train_mask = np.zeros(num_nodes, dtype=bool)\n",
    "        val_mask = np.zeros(num_nodes, dtype=bool)\n",
    "        test_mask = np.zeros(num_nodes, dtype=bool)\n",
    "        train_mask[train_index] = True\n",
    "        val_mask[val_index] = True\n",
    "        test_mask[test_index] = True\n",
    "        adjacency = self.build_adjacency(graph)\n",
    "        print(\"Node's feature shape: \", x.shape)\n",
    "        print(\"Node's label shape: \", y.shape)\n",
    "        print(\"Adjacency's shape: \", adjacency.shape)\n",
    "        print(\"Number of training nodes: \", train_mask.sum())\n",
    "        print(\"Number of validation nodes: \", val_mask.sum())\n",
    "        print(\"Number of test nodes: \", test_mask.sum())\n",
    "\n",
    "        return Data(x=x, y=y, adjacency=adjacency,\n",
    "                    train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)\n",
    "\n",
    "    @staticmethod\n",
    "    def build_adjacency(adj_dict):\n",
    "        \"\"\"根据邻接表创建邻接矩阵\"\"\"\n",
    "        edge_index = []\n",
    "        num_nodes = len(adj_dict)\n",
    "        for src, dst in adj_dict.items():\n",
    "            edge_index.extend([src, v] for v in dst)\n",
    "            edge_index.extend([v, src] for v in dst)\n",
    "        # 去除重复的边\n",
    "        edge_index = list(k for k, _ in itertools.groupby(sorted(edge_index)))\n",
    "        edge_index = np.asarray(edge_index)\n",
    "        adjacency = sp.coo_matrix((np.ones(len(edge_index)), \n",
    "                                   (edge_index[:, 0], edge_index[:, 1])),\n",
    "                    shape=(num_nodes, num_nodes), dtype=\"float32\")\n",
    "        return adjacency\n",
    "\n",
    "    @staticmethod\n",
    "    def read_data(path):\n",
    "        \"\"\"使用不同的方式读取原始数据以进一步处理\"\"\"\n",
    "        name = osp.basename(path)\n",
    "        if name == \"ind.cora.test.index\":\n",
    "            out = np.genfromtxt(path, dtype=\"int64\")\n",
    "            return out\n",
    "        else:\n",
    "            out = pickle.load(open(path, \"rb\"), encoding=\"latin1\")\n",
    "            out = out.toarray() if hasattr(out, \"toarray\") else out\n",
    "            return out\n",
    "\n",
    "    @staticmethod\n",
    "    def normalization(adjacency):\n",
    "        \"\"\"计算 L=D^-0.5 * (A+I) * D^-0.5\"\"\"\n",
    "        adjacency += sp.eye(adjacency.shape[0])    # 增加自连接\n",
    "        degree = np.array(adjacency.sum(1))\n",
    "        d_hat = sp.diags(np.power(degree, -0.5).flatten())\n",
    "        return d_hat.dot(adjacency).dot(d_hat).tocoo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357ea42d-93c1-49ab-96e3-2a9155cbbe08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d314fb1e-242e-48dc-bec4-7695d68b23af",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 定义网络：\n",
    "\n",
    "class NeighborAggregator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, \n",
    "                 use_bias=False, aggr_method=\"mean\"):\n",
    "        \"\"\"聚合节点邻居\n",
    "\n",
    "        Args:\n",
    "            input_dim: 输入特征的维度\n",
    "            output_dim: 输出特征的维度\n",
    "            use_bias: 是否使用偏置 (default: {False})\n",
    "            aggr_method: 邻居聚合方式 (default: {mean})\n",
    "        \"\"\"\n",
    "        super(NeighborAggregator, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.use_bias = use_bias\n",
    "        self.aggr_method = aggr_method\n",
    "        self.weight = nn.Parameter(torch.Tensor(input_dim, output_dim))\n",
    "        if self.use_bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(self.output_dim))\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        init.kaiming_uniform_(self.weight)\n",
    "        if self.use_bias:\n",
    "            init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, neighbor_feature):\n",
    "        if self.aggr_method == \"mean\":\n",
    "            aggr_neighbor = neighbor_feature.mean(dim=1)\n",
    "        elif self.aggr_method == \"sum\":\n",
    "            aggr_neighbor = neighbor_feature.sum(dim=1)\n",
    "        elif self.aggr_method == \"max\":\n",
    "            aggr_neighbor = neighbor_feature.max(dim=1)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown aggr type, expected sum, max, or mean, but got {}\"\n",
    "                             .format(self.aggr_method))\n",
    "        \n",
    "        neighbor_hidden = torch.matmul(aggr_neighbor, self.weight)\n",
    "        if self.use_bias:\n",
    "            neighbor_hidden += self.bias\n",
    "\n",
    "        return neighbor_hidden\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return 'in_features={}, out_features={}, aggr_method={}'.format(\n",
    "            self.input_dim, self.output_dim, self.aggr_method)\n",
    "    \n",
    "\n",
    "class SageGCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim,\n",
    "                 activation=F.relu,\n",
    "                 aggr_neighbor_method=\"mean\",\n",
    "                 aggr_hidden_method=\"sum\"):\n",
    "        \"\"\"SageGCN层定义\n",
    "\n",
    "        Args:\n",
    "            input_dim: 输入特征的维度\n",
    "            hidden_dim: 隐层特征的维度，\n",
    "                当aggr_hidden_method=sum, 输出维度为hidden_dim\n",
    "                当aggr_hidden_method=concat, 输出维度为hidden_dim*2\n",
    "            activation: 激活函数\n",
    "            aggr_neighbor_method: 邻居特征聚合方法，[\"mean\", \"sum\", \"max\"]\n",
    "            aggr_hidden_method: 节点特征的更新方法，[\"sum\", \"concat\"]\n",
    "        \"\"\"\n",
    "        super(SageGCN, self).__init__()\n",
    "        assert aggr_neighbor_method in [\"mean\", \"sum\", \"max\"]\n",
    "        assert aggr_hidden_method in [\"sum\", \"concat\"]\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.aggr_neighbor_method = aggr_neighbor_method\n",
    "        self.aggr_hidden_method = aggr_hidden_method\n",
    "        self.activation = activation\n",
    "        self.aggregator = NeighborAggregator(input_dim, hidden_dim,\n",
    "                                             aggr_method=aggr_neighbor_method)\n",
    "        self.weight = nn.Parameter(torch.Tensor(input_dim, hidden_dim))\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        init.kaiming_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, src_node_features, neighbor_node_features):\n",
    "        neighbor_hidden = self.aggregator(neighbor_node_features)\n",
    "        self_hidden = torch.matmul(src_node_features, self.weight)\n",
    "        \n",
    "        if self.aggr_hidden_method == \"sum\":\n",
    "            hidden = self_hidden + neighbor_hidden\n",
    "        elif self.aggr_hidden_method == \"concat\":\n",
    "            hidden = torch.cat([self_hidden, neighbor_hidden], dim=1)\n",
    "        else:\n",
    "            raise ValueError(\"Expected sum or concat, got {}\"\n",
    "                             .format(self.aggr_hidden))\n",
    "        if self.activation:\n",
    "            return self.activation(hidden)\n",
    "        else:\n",
    "            return hidden\n",
    "\n",
    "    def extra_repr(self):\n",
    "        output_dim = self.hidden_dim if self.aggr_hidden_method == \"sum\" else self.hidden_dim * 2\n",
    "        return 'in_features={}, out_features={}, aggr_hidden_method={}'.format(\n",
    "            self.input_dim, output_dim, self.aggr_hidden_method)\n",
    "\n",
    "\n",
    "class GraphSage(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim,\n",
    "                 num_neighbors_list):\n",
    "        super(GraphSage, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_neighbors_list = num_neighbors_list\n",
    "        self.num_layers = len(num_neighbors_list)\n",
    "        self.gcn = nn.ModuleList()\n",
    "        self.gcn.append(SageGCN(input_dim, hidden_dim[0]))\n",
    "        for index in range(0, len(hidden_dim) - 2):\n",
    "            self.gcn.append(SageGCN(hidden_dim[index], hidden_dim[index+1]))\n",
    "        self.gcn.append(SageGCN(hidden_dim[-2], hidden_dim[-1], activation=None))\n",
    "\n",
    "    def forward(self, node_features_list):\n",
    "        hidden = node_features_list\n",
    "        for l in range(self.num_layers):\n",
    "            next_hidden = []\n",
    "            gcn = self.gcn[l]\n",
    "            for hop in range(self.num_layers - l):\n",
    "                src_node_features = hidden[hop]\n",
    "                src_node_num = len(src_node_features)\n",
    "                neighbor_node_features = hidden[hop + 1] \\\n",
    "                    .view((src_node_num, self.num_neighbors_list[hop], -1))\n",
    "                h = gcn(src_node_features, neighbor_node_features)\n",
    "                next_hidden.append(h)\n",
    "            hidden = next_hidden\n",
    "        return hidden[0]\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return 'in_features={}, num_neighbors_list={}'.format(\n",
    "            self.input_dim, self.num_neighbors_list\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ac1e33-c47d-4acc-b02b-f69d25a08796",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4027b08f-2037-469b-b9dc-2c26825f154c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 定义采样的方法：\n",
    "\n",
    "def sampling(src_nodes, sample_num, neighbor_table):\n",
    "    \"\"\"根据源节点采样指定数量的邻居节点，注意使用的是有放回的采样；\n",
    "    某个节点的邻居节点数量少于采样数量时，采样结果出现重复的节点\n",
    "    \n",
    "    Arguments:\n",
    "        src_nodes {list, ndarray} -- 源节点列表\n",
    "        sample_num {int} -- 需要采样的节点数\n",
    "        neighbor_table {dict} -- 节点到其邻居节点的映射表\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray -- 采样结果构成的列表\n",
    "    \"\"\"\n",
    "    # print(\"neighbor_table type:\", type(neighbor_table))\n",
    "    results = []\n",
    "    for sid in src_nodes:\n",
    "        # 从节点的邻居中进行有放回地进行采样      \n",
    "        res = np.random.choice(\n",
    "            np.where(data.adjacency_dict.todense()[113] == 1)[1], \n",
    "            size=(sample_num, )\n",
    "        ) ## np.random.choice(neighbor_table.tocsr()[sid], size=(sample_num, )) \n",
    "        ## xmk；这里跟原来的代码有点不一样。原来的代码有问题，查了一下是因为coo类型的矩阵没法是没法index的。\n",
    "        ## 然后观察了一下，这里代码要实现的意思。所以就自己找了个方法给它修改了，就能跑了。\n",
    "        results.append(res)\n",
    "    return np.asarray(results).flatten()\n",
    "\n",
    "\n",
    "def multihop_sampling(src_nodes, sample_nums, neighbor_table):\n",
    "    \"\"\"根据源节点进行多阶采样\n",
    "    \n",
    "    Arguments:\n",
    "        src_nodes {list, np.ndarray} -- 源节点id\n",
    "        sample_nums {list of int} -- 每一阶需要采样的个数\n",
    "        neighbor_table {dict} -- 节点到其邻居节点的映射\n",
    "    \n",
    "    Returns:\n",
    "        [list of ndarray] -- 每一阶采样的结果\n",
    "\n",
    "    xmk: 多重采样的原理其实非常简单。第一步就是起始点，这里比如说是16个。\n",
    "    首先把这16个节点的list放到 sampling_result 的第一个元素的位置。\n",
    "    然后看网络有多少层，也就是要有多少跳。\n",
    "    然后如果是2跳的话，就从 sampling_result 的第一个元素开始，那就是从起始16点开始。然后对每一个节点采样10个，总共组成160个采样结果，这个结果列表放到 sampling_result 的第二个位置。\n",
    "    然后就是在第二跳的话，就去以160个点开始，每个点再采样10个，于是得到了 sampling_result 的第三个元素，也就是1600个采样点。\n",
    "    \"\"\"\n",
    "    sampling_result = [src_nodes]\n",
    "    # print(sampling_result)\n",
    "    for k, hopk_num in enumerate(sample_nums):\n",
    "        # print(sampling_result[k], type(ßsampling_result[k]))\n",
    "        hopk_result = sampling(sampling_result[k], hopk_num, neighbor_table)\n",
    "        sampling_result.append(hopk_result)\n",
    "    return sampling_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b9d82ac-1350-428a-8ba2-bb65f55e140b",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = 1433    # 输入维度\n",
    "# Note: 采样的邻居阶数需要与GCN的层数保持一致\n",
    "HIDDEN_DIM = [128, 7]   # 隐藏单元节点数\n",
    "NUM_NEIGHBORS_LIST = [10, 10]   # 每阶采样邻居的节点数\n",
    "assert len(HIDDEN_DIM) == len(NUM_NEIGHBORS_LIST)\n",
    "BTACH_SIZE = 16     # 批处理大小\n",
    "EPOCHS = 20\n",
    "NUM_BATCH_PER_EPOCH = 20    # 每个epoch循环的批次数\n",
    "LEARNING_RATE = 0.01    # 学习率\n",
    "DEVICE = \"cuda\" # if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "Data = namedtuple('Data', ['x', 'y', 'adjacency_dict',\n",
    "                           'train_mask', 'val_mask', 'test_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edfebcd7-d177-44ce-8b30-c9df46c419f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init...\n",
      "Using Cached file: D:\\forCoding_data\\ML_runCodeFromBook\\深入浅出图神经网络\\originalData\\cora\\ch5_cached.pkl\n"
     ]
    }
   ],
   "source": [
    "data = CoraData().data\n",
    "x = data.x / data.x.sum(1, keepdims=True)  # 归一化数据，使得每一行和为1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb4d76ed-990f-454c-b6b6-8d2e1c3b1293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphSage(\n",
      "  in_features=1433, num_neighbors_list=[10, 10]\n",
      "  (gcn): ModuleList(\n",
      "    (0): SageGCN(\n",
      "      in_features=1433, out_features=128, aggr_hidden_method=sum\n",
      "      (aggregator): NeighborAggregator(in_features=1433, out_features=128, aggr_method=mean)\n",
      "    )\n",
      "    (1): SageGCN(\n",
      "      in_features=128, out_features=7, aggr_hidden_method=sum\n",
      "      (aggregator): NeighborAggregator(in_features=128, out_features=7, aggr_method=mean)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "train_index = np.where(data.train_mask)[0]\n",
    "train_label = data.y\n",
    "test_index = np.where(data.test_mask)[0]\n",
    "model = GraphSage(input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM,\n",
    "                  num_neighbors_list=NUM_NEIGHBORS_LIST).to(DEVICE)\n",
    "print(model)\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9faff79-39c7-42e3-846b-600adbe2187e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf81a885-b424-4657-99d1-8af94eff18e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    for e in range(EPOCHS):\n",
    "        for batch in range(NUM_BATCH_PER_EPOCH):\n",
    "            batch_src_index = np.random.choice(train_index, size=(BTACH_SIZE,))\n",
    "            batch_src_label = torch.from_numpy(train_label[batch_src_index]).long().to(DEVICE)\n",
    "            batch_sampling_result = multihop_sampling(batch_src_index, NUM_NEIGHBORS_LIST, data.adjacency_dict)\n",
    "            ## xmk：batch_sampling_result 是一个列表的列表，采样2跳，那就有3个元素在里面。每一个元素都是列表，长度分别为16，160，1600.\n",
    "            ## 每一个列表中的节点都可以是采样点。\n",
    "            batch_sampling_x = [torch.from_numpy(x[idx]).float().to(DEVICE) for idx in batch_sampling_result]\n",
    "            ## xmk: 我看懂了它逐层操作的逻辑。换句话说就是，我看懂了GraphSage.forward方法的底细。只能说我懂作者的意图，但是作者的实现方式似乎能够有所优化。作者的操作，实现上没啥问题，\n",
    "            ## 但我直观的感觉就是，它可优化的余地蛮大。\n",
    "            batch_train_logits = model(batch_sampling_x)\n",
    "            ## xmk：其实原始的代码没有对concat做支持，concat完之后最后输出的logit的维度变成原来的两倍，但是logit并没有被以某种形式变回原来的维度。\n",
    "            loss = criterion(batch_train_logits, batch_src_label)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()  # 反向传播计算参数的梯度\n",
    "            optimizer.step()  # 使用优化方法进行梯度更新\n",
    "            print(\"Epoch {:03d} Batch {:03d} Loss: {:.4f}\".format(e, batch, loss.item()))\n",
    "        test()\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_sampling_result = multihop_sampling(test_index, NUM_NEIGHBORS_LIST, data.adjacency_dict)\n",
    "        test_x = [torch.from_numpy(x[idx]).float().to(DEVICE) for idx in test_sampling_result]\n",
    "        test_logits = model(test_x)\n",
    "        test_label = torch.from_numpy(data.y[test_index]).long().to(DEVICE)\n",
    "        predict_y = test_logits.max(1)[1]\n",
    "        accuarcy = torch.eq(predict_y, test_label).float().mean().item()\n",
    "        print(\"Test Accuracy: \", accuarcy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "668d5aa8-acda-4f95-bbf6-309db48dae33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "11*14+6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79b9decd-f57e-43ca-a217-80ab4278cb01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b6cef93-2558-4620-bba5-569c54456f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time: 2024-09-17 17:22:44.754816\n",
      "Epoch 000 Batch 000 Loss: 1.9473\n",
      "Epoch 000 Batch 001 Loss: 1.8741\n",
      "Epoch 000 Batch 002 Loss: 1.9381\n",
      "Epoch 000 Batch 003 Loss: 2.0555\n",
      "Epoch 000 Batch 004 Loss: 1.9920\n",
      "Epoch 000 Batch 005 Loss: 1.6599\n",
      "Epoch 000 Batch 006 Loss: 1.8158\n",
      "Epoch 000 Batch 007 Loss: 1.9318\n",
      "Epoch 000 Batch 008 Loss: 1.6733\n",
      "Epoch 000 Batch 009 Loss: 1.7281\n",
      "Epoch 000 Batch 010 Loss: 1.6964\n",
      "Epoch 000 Batch 011 Loss: 1.5400\n",
      "Epoch 000 Batch 012 Loss: 1.6820\n",
      "Epoch 000 Batch 013 Loss: 1.2548\n",
      "Epoch 000 Batch 014 Loss: 1.4112\n",
      "Epoch 000 Batch 015 Loss: 1.4478\n",
      "Epoch 000 Batch 016 Loss: 1.5091\n",
      "Epoch 000 Batch 017 Loss: 1.2125\n",
      "Epoch 000 Batch 018 Loss: 1.1966\n",
      "Epoch 000 Batch 019 Loss: 1.2552\n",
      "Test Accuracy:  0.3240000009536743\n",
      "Epoch 001 Batch 000 Loss: 1.2366\n",
      "Epoch 001 Batch 001 Loss: 1.1651\n",
      "Epoch 001 Batch 002 Loss: 0.9159\n",
      "Epoch 001 Batch 003 Loss: 0.8883\n",
      "Epoch 001 Batch 004 Loss: 0.8544\n",
      "Epoch 001 Batch 005 Loss: 1.0806\n",
      "Epoch 001 Batch 006 Loss: 0.9748\n",
      "Epoch 001 Batch 007 Loss: 0.8958\n",
      "Epoch 001 Batch 008 Loss: 0.7228\n",
      "Epoch 001 Batch 009 Loss: 0.6730\n",
      "Epoch 001 Batch 010 Loss: 0.8033\n",
      "Epoch 001 Batch 011 Loss: 0.7060\n",
      "Epoch 001 Batch 012 Loss: 0.5752\n",
      "Epoch 001 Batch 013 Loss: 0.6117\n",
      "Epoch 001 Batch 014 Loss: 0.4210\n",
      "Epoch 001 Batch 015 Loss: 0.5937\n",
      "Epoch 001 Batch 016 Loss: 0.3208\n",
      "Epoch 001 Batch 017 Loss: 0.3656\n",
      "Epoch 001 Batch 018 Loss: 0.3498\n",
      "Epoch 001 Batch 019 Loss: 0.2993\n",
      "Test Accuracy:  0.42900002002716064\n",
      "Epoch 002 Batch 000 Loss: 0.3874\n",
      "Epoch 002 Batch 001 Loss: 0.3105\n",
      "Epoch 002 Batch 002 Loss: 0.3503\n",
      "Epoch 002 Batch 003 Loss: 0.2444\n",
      "Epoch 002 Batch 004 Loss: 0.3542\n",
      "Epoch 002 Batch 005 Loss: 0.2877\n",
      "Epoch 002 Batch 006 Loss: 0.2070\n",
      "Epoch 002 Batch 007 Loss: 0.3164\n",
      "Epoch 002 Batch 008 Loss: 0.1942\n",
      "Epoch 002 Batch 009 Loss: 0.1951\n",
      "Epoch 002 Batch 010 Loss: 0.1495\n",
      "Epoch 002 Batch 011 Loss: 0.1217\n",
      "Epoch 002 Batch 012 Loss: 0.1199\n",
      "Epoch 002 Batch 013 Loss: 0.1216\n",
      "Epoch 002 Batch 014 Loss: 0.1248\n",
      "Epoch 002 Batch 015 Loss: 0.1602\n",
      "Epoch 002 Batch 016 Loss: 0.1775\n",
      "Epoch 002 Batch 017 Loss: 0.1063\n",
      "Epoch 002 Batch 018 Loss: 0.2327\n",
      "Epoch 002 Batch 019 Loss: 0.0718\n",
      "Test Accuracy:  0.5850000381469727\n",
      "Epoch 003 Batch 000 Loss: 0.0901\n",
      "Epoch 003 Batch 001 Loss: 0.0560\n",
      "Epoch 003 Batch 002 Loss: 0.0635\n",
      "Epoch 003 Batch 003 Loss: 0.1056\n",
      "Epoch 003 Batch 004 Loss: 0.0775\n",
      "Epoch 003 Batch 005 Loss: 0.1784\n",
      "Epoch 003 Batch 006 Loss: 0.1130\n",
      "Epoch 003 Batch 007 Loss: 0.0767\n",
      "Epoch 003 Batch 008 Loss: 0.0936\n",
      "Epoch 003 Batch 009 Loss: 0.1292\n",
      "Epoch 003 Batch 010 Loss: 0.0650\n",
      "Epoch 003 Batch 011 Loss: 0.0744\n",
      "Epoch 003 Batch 012 Loss: 0.1075\n",
      "Epoch 003 Batch 013 Loss: 0.0738\n",
      "Epoch 003 Batch 014 Loss: 0.0732\n",
      "Epoch 003 Batch 015 Loss: 0.0944\n",
      "Epoch 003 Batch 016 Loss: 0.0687\n",
      "Epoch 003 Batch 017 Loss: 0.0990\n",
      "Epoch 003 Batch 018 Loss: 0.0928\n",
      "Epoch 003 Batch 019 Loss: 0.0625\n",
      "Test Accuracy:  0.5740000009536743\n",
      "Epoch 004 Batch 000 Loss: 0.0732\n",
      "Epoch 004 Batch 001 Loss: 0.0756\n",
      "Epoch 004 Batch 002 Loss: 0.0858\n",
      "Epoch 004 Batch 003 Loss: 0.0858\n",
      "Epoch 004 Batch 004 Loss: 0.0536\n",
      "Epoch 004 Batch 005 Loss: 0.0559\n",
      "Epoch 004 Batch 006 Loss: 0.0699\n",
      "Epoch 004 Batch 007 Loss: 0.0693\n",
      "Epoch 004 Batch 008 Loss: 0.0926\n",
      "Epoch 004 Batch 009 Loss: 0.0561\n",
      "Epoch 004 Batch 010 Loss: 0.0753\n",
      "Epoch 004 Batch 011 Loss: 0.0659\n",
      "Epoch 004 Batch 012 Loss: 0.0688\n",
      "Epoch 004 Batch 013 Loss: 0.0770\n",
      "Epoch 004 Batch 014 Loss: 0.0677\n",
      "Epoch 004 Batch 015 Loss: 0.0746\n",
      "Epoch 004 Batch 016 Loss: 0.0725\n",
      "Epoch 004 Batch 017 Loss: 0.0892\n",
      "Epoch 004 Batch 018 Loss: 0.0716\n",
      "Epoch 004 Batch 019 Loss: 0.0663\n",
      "Test Accuracy:  0.5530000329017639\n",
      "Epoch 005 Batch 000 Loss: 0.0683\n",
      "Epoch 005 Batch 001 Loss: 0.0762\n",
      "Epoch 005 Batch 002 Loss: 0.0706\n",
      "Epoch 005 Batch 003 Loss: 0.0764\n",
      "Epoch 005 Batch 004 Loss: 0.0732\n",
      "Epoch 005 Batch 005 Loss: 0.0954\n",
      "Epoch 005 Batch 006 Loss: 0.0780\n",
      "Epoch 005 Batch 007 Loss: 0.0695\n",
      "Epoch 005 Batch 008 Loss: 0.0586\n",
      "Epoch 005 Batch 009 Loss: 0.0723\n",
      "Epoch 005 Batch 010 Loss: 0.0854\n",
      "Epoch 005 Batch 011 Loss: 0.0837\n",
      "Epoch 005 Batch 012 Loss: 0.0809\n",
      "Epoch 005 Batch 013 Loss: 0.0836\n",
      "Epoch 005 Batch 014 Loss: 0.0689\n",
      "Epoch 005 Batch 015 Loss: 0.0657\n",
      "Epoch 005 Batch 016 Loss: 0.1322\n",
      "Epoch 005 Batch 017 Loss: 0.0743\n",
      "Epoch 005 Batch 018 Loss: 0.0759\n",
      "Epoch 005 Batch 019 Loss: 0.0582\n",
      "Test Accuracy:  0.5800000429153442\n",
      "Epoch 006 Batch 000 Loss: 0.1001\n",
      "Epoch 006 Batch 001 Loss: 0.0685\n",
      "Epoch 006 Batch 002 Loss: 0.0537\n",
      "Epoch 006 Batch 003 Loss: 0.0635\n",
      "Epoch 006 Batch 004 Loss: 0.0568\n",
      "Epoch 006 Batch 005 Loss: 0.0655\n",
      "Epoch 006 Batch 006 Loss: 0.0769\n",
      "Epoch 006 Batch 007 Loss: 0.0926\n",
      "Epoch 006 Batch 008 Loss: 0.0706\n",
      "Epoch 006 Batch 009 Loss: 0.0857\n",
      "Epoch 006 Batch 010 Loss: 0.1047\n",
      "Epoch 006 Batch 011 Loss: 0.0774\n",
      "Epoch 006 Batch 012 Loss: 0.0754\n",
      "Epoch 006 Batch 013 Loss: 0.0721\n",
      "Epoch 006 Batch 014 Loss: 0.0587\n",
      "Epoch 006 Batch 015 Loss: 0.0594\n",
      "Epoch 006 Batch 016 Loss: 0.0665\n",
      "Epoch 006 Batch 017 Loss: 0.0773\n",
      "Epoch 006 Batch 018 Loss: 0.0883\n",
      "Epoch 006 Batch 019 Loss: 0.0772\n",
      "Test Accuracy:  0.5900000333786011\n",
      "Epoch 007 Batch 000 Loss: 0.0933\n",
      "Epoch 007 Batch 001 Loss: 0.0652\n",
      "Epoch 007 Batch 002 Loss: 0.0516\n",
      "Epoch 007 Batch 003 Loss: 0.0977\n",
      "Epoch 007 Batch 004 Loss: 0.0576\n",
      "Epoch 007 Batch 005 Loss: 0.0725\n",
      "Epoch 007 Batch 006 Loss: 0.0782\n",
      "Epoch 007 Batch 007 Loss: 0.0655\n",
      "Epoch 007 Batch 008 Loss: 0.0604\n",
      "Epoch 007 Batch 009 Loss: 0.0727\n",
      "Epoch 007 Batch 010 Loss: 0.0433\n",
      "Epoch 007 Batch 011 Loss: 0.0703\n",
      "Epoch 007 Batch 012 Loss: 0.1124\n",
      "Epoch 007 Batch 013 Loss: 0.0738\n",
      "Epoch 007 Batch 014 Loss: 0.0715\n",
      "Epoch 007 Batch 015 Loss: 0.0681\n",
      "Epoch 007 Batch 016 Loss: 0.0743\n",
      "Epoch 007 Batch 017 Loss: 0.0847\n",
      "Epoch 007 Batch 018 Loss: 0.0703\n",
      "Epoch 007 Batch 019 Loss: 0.0677\n",
      "Test Accuracy:  0.5580000281333923\n",
      "Epoch 008 Batch 000 Loss: 0.0480\n",
      "Epoch 008 Batch 001 Loss: 0.0768\n",
      "Epoch 008 Batch 002 Loss: 0.0665\n",
      "Epoch 008 Batch 003 Loss: 0.0678\n",
      "Epoch 008 Batch 004 Loss: 0.0919\n",
      "Epoch 008 Batch 005 Loss: 0.0538\n",
      "Epoch 008 Batch 006 Loss: 0.0853\n",
      "Epoch 008 Batch 007 Loss: 0.0608\n",
      "Epoch 008 Batch 008 Loss: 0.0670\n",
      "Epoch 008 Batch 009 Loss: 0.0623\n",
      "Epoch 008 Batch 010 Loss: 0.0567\n",
      "Epoch 008 Batch 011 Loss: 0.0786\n",
      "Epoch 008 Batch 012 Loss: 0.0618\n",
      "Epoch 008 Batch 013 Loss: 0.0621\n",
      "Epoch 008 Batch 014 Loss: 0.0811\n",
      "Epoch 008 Batch 015 Loss: 0.0755\n",
      "Epoch 008 Batch 016 Loss: 0.0660\n",
      "Epoch 008 Batch 017 Loss: 0.0826\n",
      "Epoch 008 Batch 018 Loss: 0.1139\n",
      "Epoch 008 Batch 019 Loss: 0.0619\n",
      "Test Accuracy:  0.581000030040741\n",
      "Epoch 009 Batch 000 Loss: 0.1051\n",
      "Epoch 009 Batch 001 Loss: 0.0808\n",
      "Epoch 009 Batch 002 Loss: 0.0751\n",
      "Epoch 009 Batch 003 Loss: 0.0595\n",
      "Epoch 009 Batch 004 Loss: 0.0547\n",
      "Epoch 009 Batch 005 Loss: 0.0619\n",
      "Epoch 009 Batch 006 Loss: 0.0798\n",
      "Epoch 009 Batch 007 Loss: 0.0779\n",
      "Epoch 009 Batch 008 Loss: 0.0639\n",
      "Epoch 009 Batch 009 Loss: 0.0647\n",
      "Epoch 009 Batch 010 Loss: 0.0669\n",
      "Epoch 009 Batch 011 Loss: 0.0815\n",
      "Epoch 009 Batch 012 Loss: 0.0700\n",
      "Epoch 009 Batch 013 Loss: 0.0799\n",
      "Epoch 009 Batch 014 Loss: 0.0717\n",
      "Epoch 009 Batch 015 Loss: 0.0628\n",
      "Epoch 009 Batch 016 Loss: 0.0604\n",
      "Epoch 009 Batch 017 Loss: 0.0549\n",
      "Epoch 009 Batch 018 Loss: 0.0791\n",
      "Epoch 009 Batch 019 Loss: 0.0784\n",
      "Test Accuracy:  0.609000027179718\n",
      "Epoch 010 Batch 000 Loss: 0.0791\n",
      "Epoch 010 Batch 001 Loss: 0.0823\n",
      "Epoch 010 Batch 002 Loss: 0.0746\n",
      "Epoch 010 Batch 003 Loss: 0.0979\n",
      "Epoch 010 Batch 004 Loss: 0.0870\n",
      "Epoch 010 Batch 005 Loss: 0.0712\n",
      "Epoch 010 Batch 006 Loss: 0.0479\n",
      "Epoch 010 Batch 007 Loss: 0.0496\n",
      "Epoch 010 Batch 008 Loss: 0.0636\n",
      "Epoch 010 Batch 009 Loss: 0.0501\n",
      "Epoch 010 Batch 010 Loss: 0.0827\n",
      "Epoch 010 Batch 011 Loss: 0.0541\n",
      "Epoch 010 Batch 012 Loss: 0.0754\n",
      "Epoch 010 Batch 013 Loss: 0.0584\n",
      "Epoch 010 Batch 014 Loss: 0.0828\n",
      "Epoch 010 Batch 015 Loss: 0.0511\n",
      "Epoch 010 Batch 016 Loss: 0.0536\n",
      "Epoch 010 Batch 017 Loss: 0.0777\n",
      "Epoch 010 Batch 018 Loss: 0.0635\n",
      "Epoch 010 Batch 019 Loss: 0.0618\n",
      "Test Accuracy:  0.5680000185966492\n",
      "Epoch 011 Batch 000 Loss: 0.0769\n",
      "Epoch 011 Batch 001 Loss: 0.0939\n",
      "Epoch 011 Batch 002 Loss: 0.0602\n",
      "Epoch 011 Batch 003 Loss: 0.0755\n",
      "Epoch 011 Batch 004 Loss: 0.0581\n",
      "Epoch 011 Batch 005 Loss: 0.0588\n",
      "Epoch 011 Batch 006 Loss: 0.0590\n",
      "Epoch 011 Batch 007 Loss: 0.0599\n",
      "Epoch 011 Batch 008 Loss: 0.0984\n",
      "Epoch 011 Batch 009 Loss: 0.0889\n",
      "Epoch 011 Batch 010 Loss: 0.0491\n",
      "Epoch 011 Batch 011 Loss: 0.0515\n",
      "Epoch 011 Batch 012 Loss: 0.0635\n",
      "Epoch 011 Batch 013 Loss: 0.0589\n",
      "Epoch 011 Batch 014 Loss: 0.0564\n",
      "Epoch 011 Batch 015 Loss: 0.0644\n",
      "Epoch 011 Batch 016 Loss: 0.0676\n",
      "Epoch 011 Batch 017 Loss: 0.0891\n",
      "Epoch 011 Batch 018 Loss: 0.0889\n",
      "Epoch 011 Batch 019 Loss: 0.0777\n",
      "Test Accuracy:  0.5710000395774841\n",
      "Epoch 012 Batch 000 Loss: 0.0650\n",
      "Epoch 012 Batch 001 Loss: 0.0664\n",
      "Epoch 012 Batch 002 Loss: 0.0532\n",
      "Epoch 012 Batch 003 Loss: 0.0810\n",
      "Epoch 012 Batch 004 Loss: 0.0528\n",
      "Epoch 012 Batch 005 Loss: 0.0694\n",
      "Epoch 012 Batch 006 Loss: 0.0568\n",
      "Epoch 012 Batch 007 Loss: 0.0635\n",
      "Epoch 012 Batch 008 Loss: 0.0436\n",
      "Epoch 012 Batch 009 Loss: 0.0607\n",
      "Epoch 012 Batch 010 Loss: 0.0471\n",
      "Epoch 012 Batch 011 Loss: 0.0482\n",
      "Epoch 012 Batch 012 Loss: 0.0459\n",
      "Epoch 012 Batch 013 Loss: 0.0485\n",
      "Epoch 012 Batch 014 Loss: 0.0449\n",
      "Epoch 012 Batch 015 Loss: 0.0387\n",
      "Epoch 012 Batch 016 Loss: 0.0610\n",
      "Epoch 012 Batch 017 Loss: 0.0835\n",
      "Epoch 012 Batch 018 Loss: 0.0657\n",
      "Epoch 012 Batch 019 Loss: 0.0455\n",
      "Test Accuracy:  0.5660000443458557\n",
      "Epoch 013 Batch 000 Loss: 0.0603\n",
      "Epoch 013 Batch 001 Loss: 0.0848\n",
      "Epoch 013 Batch 002 Loss: 0.0488\n",
      "Epoch 013 Batch 003 Loss: 0.0650\n",
      "Epoch 013 Batch 004 Loss: 0.0815\n",
      "Epoch 013 Batch 005 Loss: 0.0671\n",
      "Epoch 013 Batch 006 Loss: 0.0664\n",
      "Epoch 013 Batch 007 Loss: 0.0754\n",
      "Epoch 013 Batch 008 Loss: 0.0545\n",
      "Epoch 013 Batch 009 Loss: 0.0658\n",
      "Epoch 013 Batch 010 Loss: 0.0478\n",
      "Epoch 013 Batch 011 Loss: 0.0499\n",
      "Epoch 013 Batch 012 Loss: 0.0535\n",
      "Epoch 013 Batch 013 Loss: 0.0854\n",
      "Epoch 013 Batch 014 Loss: 0.0794\n",
      "Epoch 013 Batch 015 Loss: 0.0775\n",
      "Epoch 013 Batch 016 Loss: 0.0780\n",
      "Epoch 013 Batch 017 Loss: 0.0677\n",
      "Epoch 013 Batch 018 Loss: 0.0624\n",
      "Epoch 013 Batch 019 Loss: 0.1031\n",
      "Test Accuracy:  0.5760000348091125\n",
      "Epoch 014 Batch 000 Loss: 0.0555\n",
      "Epoch 014 Batch 001 Loss: 0.0694\n",
      "Epoch 014 Batch 002 Loss: 0.0743\n",
      "Epoch 014 Batch 003 Loss: 0.0779\n",
      "Epoch 014 Batch 004 Loss: 0.0562\n",
      "Epoch 014 Batch 005 Loss: 0.0482\n",
      "Epoch 014 Batch 006 Loss: 0.0605\n",
      "Epoch 014 Batch 007 Loss: 0.0966\n",
      "Epoch 014 Batch 008 Loss: 0.0731\n",
      "Epoch 014 Batch 009 Loss: 0.0604\n",
      "Epoch 014 Batch 010 Loss: 0.0597\n",
      "Epoch 014 Batch 011 Loss: 0.0549\n",
      "Epoch 014 Batch 012 Loss: 0.0656\n",
      "Epoch 014 Batch 013 Loss: 0.0618\n",
      "Epoch 014 Batch 014 Loss: 0.0655\n",
      "Epoch 014 Batch 015 Loss: 0.0767\n",
      "Epoch 014 Batch 016 Loss: 0.0730\n",
      "Epoch 014 Batch 017 Loss: 0.0600\n",
      "Epoch 014 Batch 018 Loss: 0.0747\n",
      "Epoch 014 Batch 019 Loss: 0.0546\n",
      "Test Accuracy:  0.6220000386238098\n",
      "Epoch 015 Batch 000 Loss: 0.0718\n",
      "Epoch 015 Batch 001 Loss: 0.0637\n",
      "Epoch 015 Batch 002 Loss: 0.0875\n",
      "Epoch 015 Batch 003 Loss: 0.0591\n",
      "Epoch 015 Batch 004 Loss: 0.0458\n",
      "Epoch 015 Batch 005 Loss: 0.0745\n",
      "Epoch 015 Batch 006 Loss: 0.0813\n",
      "Epoch 015 Batch 007 Loss: 0.0705\n",
      "Epoch 015 Batch 008 Loss: 0.0418\n",
      "Epoch 015 Batch 009 Loss: 0.0537\n",
      "Epoch 015 Batch 010 Loss: 0.0728\n",
      "Epoch 015 Batch 011 Loss: 0.0799\n",
      "Epoch 015 Batch 012 Loss: 0.0470\n",
      "Epoch 015 Batch 013 Loss: 0.0534\n",
      "Epoch 015 Batch 014 Loss: 0.0659\n",
      "Epoch 015 Batch 015 Loss: 0.0732\n",
      "Epoch 015 Batch 016 Loss: 0.0477\n",
      "Epoch 015 Batch 017 Loss: 0.0647\n",
      "Epoch 015 Batch 018 Loss: 0.0835\n",
      "Epoch 015 Batch 019 Loss: 0.0636\n",
      "Test Accuracy:  0.6020000576972961\n",
      "Epoch 016 Batch 000 Loss: 0.0368\n",
      "Epoch 016 Batch 001 Loss: 0.0504\n",
      "Epoch 016 Batch 002 Loss: 0.0640\n",
      "Epoch 016 Batch 003 Loss: 0.0481\n",
      "Epoch 016 Batch 004 Loss: 0.0394\n",
      "Epoch 016 Batch 005 Loss: 0.0604\n",
      "Epoch 016 Batch 006 Loss: 0.0504\n",
      "Epoch 016 Batch 007 Loss: 0.1093\n",
      "Epoch 016 Batch 008 Loss: 0.0693\n",
      "Epoch 016 Batch 009 Loss: 0.0568\n",
      "Epoch 016 Batch 010 Loss: 0.0697\n",
      "Epoch 016 Batch 011 Loss: 0.0655\n",
      "Epoch 016 Batch 012 Loss: 0.0622\n",
      "Epoch 016 Batch 013 Loss: 0.0567\n",
      "Epoch 016 Batch 014 Loss: 0.0952\n",
      "Epoch 016 Batch 015 Loss: 0.0665\n",
      "Epoch 016 Batch 016 Loss: 0.0695\n",
      "Epoch 016 Batch 017 Loss: 0.0752\n",
      "Epoch 016 Batch 018 Loss: 0.0473\n",
      "Epoch 016 Batch 019 Loss: 0.0496\n",
      "Test Accuracy:  0.5760000348091125\n",
      "Epoch 017 Batch 000 Loss: 0.0750\n",
      "Epoch 017 Batch 001 Loss: 0.0603\n",
      "Epoch 017 Batch 002 Loss: 0.0517\n",
      "Epoch 017 Batch 003 Loss: 0.0881\n",
      "Epoch 017 Batch 004 Loss: 0.0652\n",
      "Epoch 017 Batch 005 Loss: 0.0594\n",
      "Epoch 017 Batch 006 Loss: 0.0541\n",
      "Epoch 017 Batch 007 Loss: 0.0840\n",
      "Epoch 017 Batch 008 Loss: 0.0463\n",
      "Epoch 017 Batch 009 Loss: 0.0394\n",
      "Epoch 017 Batch 010 Loss: 0.0604\n",
      "Epoch 017 Batch 011 Loss: 0.0751\n",
      "Epoch 017 Batch 012 Loss: 0.0587\n",
      "Epoch 017 Batch 013 Loss: 0.0572\n",
      "Epoch 017 Batch 014 Loss: 0.0589\n",
      "Epoch 017 Batch 015 Loss: 0.0623\n",
      "Epoch 017 Batch 016 Loss: 0.1003\n",
      "Epoch 017 Batch 017 Loss: 0.0681\n",
      "Epoch 017 Batch 018 Loss: 0.0611\n",
      "Epoch 017 Batch 019 Loss: 0.0786\n",
      "Test Accuracy:  0.5830000042915344\n",
      "Epoch 018 Batch 000 Loss: 0.0430\n",
      "Epoch 018 Batch 001 Loss: 0.0705\n",
      "Epoch 018 Batch 002 Loss: 0.0607\n",
      "Epoch 018 Batch 003 Loss: 0.1066\n",
      "Epoch 018 Batch 004 Loss: 0.0640\n",
      "Epoch 018 Batch 005 Loss: 0.0570\n",
      "Epoch 018 Batch 006 Loss: 0.0577\n",
      "Epoch 018 Batch 007 Loss: 0.0707\n",
      "Epoch 018 Batch 008 Loss: 0.0660\n",
      "Epoch 018 Batch 009 Loss: 0.0566\n",
      "Epoch 018 Batch 010 Loss: 0.0772\n",
      "Epoch 018 Batch 011 Loss: 0.0490\n",
      "Epoch 018 Batch 012 Loss: 0.0604\n",
      "Epoch 018 Batch 013 Loss: 0.0795\n",
      "Epoch 018 Batch 014 Loss: 0.0570\n",
      "Epoch 018 Batch 015 Loss: 0.0798\n",
      "Epoch 018 Batch 016 Loss: 0.0527\n",
      "Epoch 018 Batch 017 Loss: 0.0813\n",
      "Epoch 018 Batch 018 Loss: 0.0559\n",
      "Epoch 018 Batch 019 Loss: 0.0669\n",
      "Test Accuracy:  0.6070000529289246\n",
      "Epoch 019 Batch 000 Loss: 0.1015\n",
      "Epoch 019 Batch 001 Loss: 0.0934\n",
      "Epoch 019 Batch 002 Loss: 0.0662\n",
      "Epoch 019 Batch 003 Loss: 0.0573\n",
      "Epoch 019 Batch 004 Loss: 0.0602\n",
      "Epoch 019 Batch 005 Loss: 0.0475\n",
      "Epoch 019 Batch 006 Loss: 0.0471\n",
      "Epoch 019 Batch 007 Loss: 0.0948\n",
      "Epoch 019 Batch 008 Loss: 0.0625\n",
      "Epoch 019 Batch 009 Loss: 0.0608\n",
      "Epoch 019 Batch 010 Loss: 0.0439\n",
      "Epoch 019 Batch 011 Loss: 0.0398\n",
      "Epoch 019 Batch 012 Loss: 0.0633\n",
      "Epoch 019 Batch 013 Loss: 0.0352\n",
      "Epoch 019 Batch 014 Loss: 0.0772\n",
      "Epoch 019 Batch 015 Loss: 0.0598\n",
      "Epoch 019 Batch 016 Loss: 0.0402\n",
      "Epoch 019 Batch 017 Loss: 0.0469\n",
      "Epoch 019 Batch 018 Loss: 0.0487\n",
      "Epoch 019 Batch 019 Loss: 0.0700\n",
      "Test Accuracy:  0.5770000219345093\n",
      "start time: 2024-09-17 17:22:44.754816\n",
      "end time 2024-09-17 17:49:01.661908\n"
     ]
    }
   ],
   "source": [
    "with TimerContext(): \n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e788b37-4d7f-4105-8652-6005aed2c048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Sep 17 17:59:58 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 551.23                 Driver Version: 551.23         CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4070 ...  WDDM  |   00000000:01:00.0  On |                  N/A |\n",
      "|  0%   32C    P8              7W /  285W |    1450MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      1900    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A      4476    C+G   ...soft Game Launcher\\UplayWebCore.exe      N/A      |\n",
      "|    0   N/A  N/A      5464    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
      "|    0   N/A  N/A      6336    C+G   ...\\cef\\cef.win7x64\\steamwebhelper.exe      N/A      |\n",
      "|    0   N/A  N/A      6796    C+G   ..._8wekyb3d8bbwe\\Microsoft.Photos.exe      N/A      |\n",
      "|    0   N/A  N/A      7844    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A      8000    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
      "|    0   N/A  N/A      8068    C+G   ...2txyewy\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A      8140    C+G   ...siveControlPanel\\SystemSettings.exe      N/A      |\n",
      "|    0   N/A  N/A      8568    C+G   ...Desktop\\app-3.4.5\\GitHubDesktop.exe      N/A      |\n",
      "|    0   N/A  N/A      9432    C+G   ...inaries\\Win64\\EpicGamesLauncher.exe      N/A      |\n",
      "|    0   N/A  N/A     11388    C+G   ...ne\\Binaries\\Win64\\EpicWebHelper.exe      N/A      |\n",
      "|    0   N/A  N/A     11832    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A     13328    C+G   ...on\\128.0.2739.79\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A     15536    C+G   ...cal\\Microsoft\\OneDrive\\OneDrive.exe      N/A      |\n",
      "|    0   N/A  N/A     15976    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A     18144    C+G   ...\\cef\\cef.win7x64\\steamwebhelper.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7dcbb2-e238-45d3-8367-231a410541a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b456721c-d466-48a5-a649-d952be5a6431",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc252ac-bce5-48b5-a69e-8399b5e9acea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830bb8a2-8ea3-4a33-99bc-fdc5211c47fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dcf601-bc7c-4531-a932-38850bdba4fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daeab963-5c63-49fe-a05c-214548b72454",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff733b7-40a9-464d-b0c3-3d4728d416ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa59d2a9-9988-4ebd-bf9a-2576d8ccd414",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa8a29c-0d88-4a01-b754-53518c37ac7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63903902-5806-42bf-83d8-9e4c42a878cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567eaba0-189d-447a-9e83-af7e3f27462f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
