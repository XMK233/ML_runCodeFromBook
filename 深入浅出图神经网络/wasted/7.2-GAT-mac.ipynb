{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d829fb2d-76b9-4bd9-9f30-aa5209b0d61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "storage dir: /Users/minkexiu/Downloads/GitHub/ML_runCodeFromBook/深入浅出图神经网络\n",
      "code dir: /Users/minkexiu/Documents/GitHub/ML_runCodeFromBook/深入浅出图神经网络\n"
     ]
    }
   ],
   "source": [
    "import random, os, tqdm, time, json, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "import sys\n",
    "random.seed(618)\n",
    "np.random.seed(907)\n",
    "\n",
    "# sys.path.append(\"../../\")\n",
    "# new_base_path = os.path.join(\n",
    "#     \"/mnt/d/forCoding_data/\",\n",
    "#     \"/\".join(\n",
    "#         os.getcwd().split(\"/\")[-1*(len(sys.path[-1].split(\"/\")) - 1):]\n",
    "#     ),\n",
    "# )\n",
    "sys.path.append(\"../../../\")\n",
    "new_base_path = os.path.join(\n",
    "    \"/Users/minkexiu/Downloads/\",\n",
    "    \"/\".join(\n",
    "        os.getcwd().split(\"/\")[-1*(len(sys.path[-1].split(\"/\")) - 1):]\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"storage dir:\", new_base_path)\n",
    "print(\"code dir:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5de6438-bd99-4374-bdb5-e9b46bf26b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 14 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:165: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:165: SyntaxWarning: invalid escape sequence '\\d'\n",
      "/var/folders/s1/1jpfx0m52rj4k7cgqkh7g3q40000gn/T/ipykernel_37912/2849389036.py:165: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  *[int(x) for x in re.findall(\"\\d+\", zh_date_str)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>雷水</th>\n",
       "      <th>水火</th>\n",
       "      <th>雷地</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>上卦</th>\n",
       "      <td>☳震木</td>\n",
       "      <td>☵坎水</td>\n",
       "      <td>☳震木</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>下卦</th>\n",
       "      <td>☵坎水</td>\n",
       "      <td>☲离火</td>\n",
       "      <td>☷坤土</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     雷水   水火   雷地\n",
       "上卦  ☳震木  ☵坎水  ☳震木\n",
       "下卦  ☵坎水  ☲离火  ☷坤土"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08 18 8 未时\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>地泽</th>\n",
       "      <th>地雷</th>\n",
       "      <th>地雷</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>上卦</th>\n",
       "      <td>☷坤土</td>\n",
       "      <td>☷坤土</td>\n",
       "      <td>☷坤土</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>下卦</th>\n",
       "      <td>☱兑金</td>\n",
       "      <td>☳震木</td>\n",
       "      <td>☳震木</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     地泽   地雷   地雷\n",
       "上卦  ☷坤土  ☷坤土  ☷坤土\n",
       "下卦  ☱兑金  ☳震木  ☳震木"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('110000', '100000', '100000')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 创建文件夹。\n",
    "if not os.path.exists(new_base_path):\n",
    "    os.makedirs(\n",
    "        new_base_path\n",
    "    )\n",
    "if not os.path.exists(os.path.join(new_base_path, \"preprocessedData\")):\n",
    "    os.makedirs(\n",
    "        os.path.join(new_base_path, \"preprocessedData\")\n",
    "    )\n",
    "if not os.path.exists(os.path.join(new_base_path, \"originalData\")):\n",
    "    os.makedirs(\n",
    "        os.path.join(new_base_path, \"originalData\")\n",
    "    )\n",
    "if not os.path.exists(os.path.join(new_base_path, \"trained_models\")):\n",
    "    os.makedirs(\n",
    "        os.path.join(new_base_path, \"trained_models\")\n",
    "    )\n",
    "\n",
    "def create_originalData_path(filename_or_path):\n",
    "    return os.path.join(new_base_path, \"originalData\", filename_or_path)\n",
    "def create_preprocessedData_path(filename_or_path):\n",
    "    return os.path.join(new_base_path, \"preprocessedData\", filename_or_path)\n",
    "def create_trained_models_path(filename_or_path):\n",
    "    return os.path.join(new_base_path, \"trained_models\", filename_or_path)\n",
    "\n",
    "def millisec2datetime(timestamp):\n",
    "    time_local = time.localtime(timestamp/1000)\n",
    "    return time.strftime(\"%Y-%m-%d %H:%M:%S\", time_local)\n",
    "    \n",
    "def run_finish():\n",
    "    # 假设你的字体文件是 'myfont.ttf' 并且位于当前目录下  \n",
    "    font = FontProperties(fname=\"/Users/minkexiu/Documents/GitHub/ML_Tryout/SimHei.ttf\", size=24)  \n",
    "    # 创建一个空白的图形  \n",
    "    fig, ax = plt.subplots()  \n",
    "    ax.imshow(\n",
    "        plt.imread(\"/Users/minkexiu/Downloads/wallhaven-dgxpyg.jpg\")\n",
    "    )\n",
    "    # 在图形中添加文字  \n",
    "    ax.text(\n",
    "        ax.get_xlim()[1] * 0.5, \n",
    "        ax.get_ylim()[0] * 0.5, \n",
    "        f\"程序于这个点跑完：\\n{millisec2datetime(time.time()*1000)}\", fontproperties=font, ha=\"center\", va=\"center\", color=\"red\"\n",
    "    )  \n",
    "    # 设置图形的布局  \n",
    "    # ax.set_xlim(0, 1)  \n",
    "    # ax.set_ylim(0, 1)  \n",
    "    ax.set_xticks([])  \n",
    "    ax.set_yticks([])  \n",
    "    ax.patch.set_color(\"blue\")\n",
    "    # 显示图形  \n",
    "    plt.show()\n",
    "        \n",
    "tqdm.tqdm.pandas() ## 引入这个，就可以在apply的时候用progress_apply了。\n",
    "\n",
    "import IPython\n",
    "def kill_current_kernel():\n",
    "    '''杀死当前的kernel释放内存空间。'''\n",
    "    IPython.Application.instance().kernel.do_shutdown(True) \n",
    "    \n",
    "def simply_show_data(df1):\n",
    "    print(df1.shape)\n",
    "    display(df1.head())\n",
    "    \n",
    "def wait_flag(saved_flag_path, time_interval_sec=10):\n",
    "    print(\"waiting for\", saved_flag_path)\n",
    "    time_count = 0\n",
    "    while True:\n",
    "        if os.path.exists(saved_flag_path):\n",
    "            break\n",
    "        time.sleep(time_interval_sec)\n",
    "        time_count+=time_interval_sec\n",
    "        print(time_count, end=\" \")\n",
    "    print(\"finish!!\")\n",
    "\n",
    "def parallelly_run_multiple_similar_python_code(codes, nb_workers = 4):\n",
    "    '''\n",
    "    codes是多条相似的python代码。\n",
    "    这个函数的作用就是将其平行地跑，每一条python代码就对应一个线程。或许可以后续优化，比如固定线程数为一个特定值。\n",
    "    nb_workers 如果赋值为\n",
    "    '''\n",
    "    assert (isinstance(nb_workers, int)), \"`nb_workers' should be int.\"\n",
    "    df_sqls = pd.DataFrame(\n",
    "        {\n",
    "            \"func\": codes\n",
    "\n",
    "        }\n",
    "    )\n",
    "    display(df_sqls)\n",
    "    from pandarallel import pandarallel\n",
    "    pandarallel.initialize(nb_workers = df_sqls.shape[0] if nb_workers<0 else nb_workers, progress_bar = True)\n",
    "    def run_sql_prlly(row):\n",
    "        try: \n",
    "            cmd = f'{row[\"func\"]}'\n",
    "            print(cmd, \"\\n\")\n",
    "            eval(cmd)\n",
    "            return \"0-success\"\n",
    "        except Exception as e:\n",
    "            return e\n",
    "    df_sqls[\"run_rsts\"] = df_sqls.parallel_apply(lambda row: run_sql_prlly(row), axis = 1)\n",
    "    display(df_sqls)\n",
    "    \n",
    "def create_originalData_path(filename_or_path):\n",
    "    return os.path.join(new_base_path, \"originalData\", filename_or_path)\n",
    "def create_preprocessedData_path(filename_or_path):\n",
    "    return os.path.join(new_base_path, \"preprocessedData\", filename_or_path)\n",
    "def create_trained_models_path(filename_or_path):\n",
    "    return os.path.join(new_base_path, \"trained_models\", filename_or_path)\n",
    "    \n",
    "class TimerContext:  \n",
    "    def __enter__(self):  \n",
    "        self.start_time = str(datetime.now())\n",
    "        print(\"start time:\", self.start_time)\n",
    "        return self  \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):  \n",
    "        print(\"start time:\", self.start_time)\n",
    "        print(\"end time\", str(datetime.now()))\n",
    "\n",
    "def three_num_get_gua(a, b, c):\n",
    "    '''梅花易数三数起卦，以取本、互、变。'''\n",
    "    bagua = [\"111\", \"110\", \"101\", \"100\", \"011\", \"010\", \"001\", \"000\"]\n",
    "    guatu = {\n",
    "        \"111\": (\"☰\", \"天\", \"乾金\"), \n",
    "        \"110\": (\"☱\", \"泽\", \"兑金\"),\n",
    "        \"101\": (\"☲\", \"火\", \"离火\"),\n",
    "        \"100\": (\"☳\" , \"雷\", \"震木\"),\n",
    "        \"011\": (\"☴\", \"风\", \"巽木\"),\n",
    "        \"010\": (\"☵\", \"水\", \"坎水\"),\n",
    "        \"001\": (\"☶\", \"山\", \"艮土\"),\n",
    "        \"000\": (\"☷\", \"地\", \"坤土\"),\n",
    "    }\n",
    "    shanggua_idx = 7 if (a % 8 == 0) else (a % 8 - 1)\n",
    "    xiagua_idx = 7 if (b % 8 == 0) else (b % 8 - 1)\n",
    "    bianyao_idx = 5 if (c % 6 == 0) else (c % 6 - 1)\n",
    "    bengua = bagua[xiagua_idx] + bagua[shanggua_idx]\n",
    "    hugua = bengua[1:-1][:3] + bengua[1:-1][1:]\n",
    "    biangua = list(bengua)\n",
    "    biangua[bianyao_idx] = str(1 - int(biangua[bianyao_idx]))\n",
    "    biangua = \"\".join(biangua)\n",
    "    df = pd.DataFrame([[\n",
    "        guatu[bengua[3:]][0]+guatu[bengua[3:]][2], guatu[hugua[3:]][0]+guatu[hugua[3:]][2], guatu[biangua[3:]][0]+guatu[biangua[3:]][2], \n",
    "    ],[\n",
    "        guatu[bengua[:3]][0]+guatu[bengua[:3]][2], guatu[hugua[:3]][0]+guatu[hugua[:3]][2], guatu[biangua[:3]][0]+guatu[biangua[:3]][2], \n",
    "    ]], index=[\"上卦\", \"下卦\"], columns = [\n",
    "        guatu[bengua[3:]][1] + guatu[bengua[:3]][1],\n",
    "        guatu[hugua[3:]][1] + guatu[hugua[:3]][1],\n",
    "        guatu[biangua[3:]][1] + guatu[biangua[:3]][1],\n",
    "    ])\n",
    "    display(df)\n",
    "    return bengua, hugua, biangua\n",
    "    \n",
    "def easy_start_gua():\n",
    "    \"\"\"用公历的日、时、分来起卦。\"\"\"\n",
    "    n1, n2, n3 = str(datetime.now())[8:10], str(datetime.now())[11:13], str(datetime.now())[14:16]\n",
    "    print(n1, n2, n3)\n",
    "    return three_num_get_gua(int(n1), int(n2), int(n3))\n",
    "easy_start_gua()\n",
    "\n",
    "import zhdate\n",
    "def easy_start_gua_lunar():\n",
    "    '''用农历的月、日、时辰来起卦。'''\n",
    "    time_now = datetime.now()\n",
    "    zh_date_str = str(zhdate.ZhDate.from_datetime(time_now))\n",
    "    zh_date_str_1 = datetime.strftime(\n",
    "        datetime(\n",
    "            *[int(x) for x in re.findall(\"\\d+\", zh_date_str)]\n",
    "        ),\n",
    "        '%Y-%m-%d'\n",
    "    )\n",
    "    zh_hour = (time_now.hour + 1)//2%12+1\n",
    "    zh_hour_dizhi = \"子、丑、寅、卯、辰、巳、午、未、申、酉、戌、亥\".split(\"、\")[zh_hour-1]\n",
    "    \n",
    "    n1, n2, n3 = zh_date_str_1[5:7], zh_date_str_1[8:10], zh_hour\n",
    "    print(n1, n2, n3, f\"{zh_hour_dizhi}时\")\n",
    "    return three_num_get_gua(int(n1), int(n2), int(n3))\n",
    "easy_start_gua_lunar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c4244b4-2925-49d0-b209-ae268e9ed32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 参考代码：https://developer.aliyun.com/article/1208980\n",
    "## 数据来源：https://github.com/tkipf/pygcn/tree/master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "948de3c7-0ab4-470e-beec-d01c25b4fd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path # 引入提升路径的兼容性\n",
    "# 引入矩阵运算的相关库\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import coo_matrix,csr_matrix,diags,eye\n",
    "# 引入深度学习框架库\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "# 引入绘图库\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9ab8205-0ba1-4120-abcf-3175e88933f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "# 1.1 导入基础模块，并设置运行环境\n",
    "# 输出计算资源情况\n",
    "device = torch.device('mps') # torch.device('cuda')if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device) # 输出 cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5462c8c6-2007-4720-917a-db1053f462fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/minkexiu/Downloads/GitHub/ML_runCodeFromBook/深入浅出图神经网络/originalData/cora\n"
     ]
    }
   ],
   "source": [
    "# 输出样本路径\n",
    "path = Path(create_originalData_path(\"cora\"))\n",
    "print(path) # 输出 cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aeb81483-e996-4e36-b1f3-071f23e50869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/minkexiu/Downloads/GitHub/ML_runCodeFromBook/深入浅出图神经网络/originalData/cora/cora.content')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path/'cora.content'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "609a7ab3-6bb7-415e-86a6-71b96aa46a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['31336' '0' '0' ... '0' '0' 'Neural_Networks']\n",
      " ['1061127' '0' '0' ... '0' '0' 'Rule_Learning']\n",
      " ['1106406' '0' '0' ... '0' '0' 'Reinforcement_Learning']\n",
      " ...\n",
      " ['1128978' '0' '0' ... '0' '0' 'Genetic_Algorithms']\n",
      " ['117328' '0' '0' ... '0' '0' 'Case_Based']\n",
      " ['24043' '0' '0' ... '0' '0' 'Neural_Networks']] (2708, 1435)\n"
     ]
    }
   ],
   "source": [
    "# 1.2 读取并解析论文数据\n",
    "# 读取论文内容数据，将其转化为数据\n",
    "paper_features_label = np.genfromtxt(path/'cora.content',dtype=np.str_) # 使用Path对象的路径构造，实例化的内容为cora.content。path/'cora.content'表示路径为'data/cora/cora.content'的字符串\n",
    "print(paper_features_label,np.shape(paper_features_label)) # 打印数据集内容与数据的形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3a85281-dc5f-4cac-ba27-f4014927419e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "论文ID序列： [  31336 1061127 1106406 ... 1128978  117328   24043]\n"
     ]
    }
   ],
   "source": [
    "# 取出数据集中的第一列:论文ID\n",
    "papers = paper_features_label[:,0].astype(np.int32)\n",
    "print(\"论文ID序列：\",papers) # 输出所有论文ID\n",
    "# 论文重新编号，并将其映射到论文ID中，实现论文的统一管理\n",
    "paper2idx = {k:v for v,k in enumerate(papers)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b8f0e1c-2fbd-4148-a1c1-3c94397f5063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "字标签矩阵的形状： (2708, 1433)\n"
     ]
    }
   ],
   "source": [
    "# 将数据中间部分的字标签取出，转化成矩阵\n",
    "features = csr_matrix(paper_features_label[:,1:-1],dtype=np.float32)\n",
    "print(\"字标签矩阵的形状：\",np.shape(features)) # 字标签矩阵的形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a77334aa-8865-4070-9e8e-17955332872f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "论文类别的索引号： {'Case_Based': 0, 'Genetic_Algorithms': 1, 'Neural_Networks': 2, 'Probabilistic_Methods': 3, 'Reinforcement_Learning': 4, 'Rule_Learning': 5, 'Theory': 6} [2, 5, 4, 4, 3]\n"
     ]
    }
   ],
   "source": [
    "# 将数据的最后一项的文章分类属性取出，转化为分类的索引\n",
    "labels = paper_features_label[:,-1]\n",
    "lbl2idx = { k:v for v,k in enumerate(sorted(np.unique(labels)))}\n",
    "labels = [lbl2idx[e] for e in labels]\n",
    "print(\"论文类别的索引号：\",lbl2idx,labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2559c8c6-a933-4a8f-9730-33d8157c907b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[     35    1033]\n",
      " [     35  103482]\n",
      " [     35  103515]\n",
      " ...\n",
      " [ 853118 1140289]\n",
      " [ 853155  853118]\n",
      " [ 954315 1155073]] (5429, 2)\n",
      "新编号节点间的对应关系： [[ 163  402]\n",
      " [ 163  659]\n",
      " [ 163 1696]\n",
      " ...\n",
      " [1887 2258]\n",
      " [1902 1887]\n",
      " [ 837 1686]] (5429, 2)\n"
     ]
    }
   ],
   "source": [
    "# 1.3 读取并解析论文关系数据\n",
    "# 读取论文关系数据，并将其转化为数据\n",
    "edges = np.genfromtxt(path/'cora.cites',dtype=np.int32) # 将数据集中论文的引用关系以数据的形式读入\n",
    "print(edges,np.shape(edges))\n",
    "# 转化为新编号节点间的关系：将数据集中论文ID表示的关系转化为重新编号后的关系\n",
    "edges = np.asarray([paper2idx[e] for e in edges.flatten()],np.int32).reshape(edges.shape)\n",
    "print(\"新编号节点间的对应关系：\",edges,edges.shape)\n",
    "# 计算邻接矩阵，行与列都是论文个数：由论文引用关系所表示的图结构生成邻接矩阵。\n",
    "adj = coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),shape=(len(labels), len(labels)), dtype=np.float32)\n",
    "# 生成无向图对称矩阵：将有向图的邻接矩阵转化为无向图的邻接矩阵。Tip：转化为无向图的原因：主要用于对论文的分类，论文的引用关系主要提供单个特征之间的关联，故更看重是不是有关系，所以无向图即可。\n",
    "adj_long = adj.multiply(adj.T < adj)\n",
    "adj = adj_long + adj_long.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ec782ea-f6fb-42eb-a025-aacbd13dbc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4 加工图结构的矩阵数据\n",
    "def normalize_adj(mx):\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum,-0.5).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.0\n",
    "    r_mat_inv = diags(r_inv)\n",
    "    return mx.dot(r_mat_inv).transpose().dot(r_mat_inv) # 兑成归一化拉普拉斯矩阵实现邻接矩阵的转化\n",
    " \n",
    "adj = normalize_adj(adj + eye(adj.shape[0])) # 对邻接矩阵进行转化对称归一化拉普拉斯矩阵转化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e9ac473-4edb-4b66-a80a-2055ffbf378b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.5 将数据转化为张量，并分配运算资源\n",
    "adj = torch.FloatTensor(adj.todense()) # 节点间关系 todense()方法将其转换回稠密矩阵。\n",
    "features = torch.FloatTensor(features.todense()) # 节点自身的特征\n",
    "labels = torch.LongTensor(labels) # 对每个节点的分类标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95e7ad2a-cf2c-4d39-8851-17a1d860d4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分数据集\n",
    "n_train = 200 # 训练数据集大小\n",
    "n_val = 300 # 验证数据集大小\n",
    "n_test = len(features) - n_train - n_val # 测试数据集大小\n",
    "np.random.seed(34)\n",
    "idxs = np.random.permutation(len(features)) # 将原有的索引打乱顺序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29baf21e-5665-4201-b2de-957faf7deed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算每个数据集的索引\n",
    "idx_train = torch.LongTensor(idxs[:n_train]) # 根据指定训练数据集的大小并划分出其对应的训练数据集索引\n",
    "idx_val = torch.LongTensor(idxs[n_train:n_train+n_val])# 根据指定验证数据集的大小并划分出其对应的验证数据集索引\n",
    "idx_test = torch.LongTensor(idxs[n_train+n_val:])# 根据指定测试数据集的大小并划分出其对应的测试数据集索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6136f78-87f2-4c9d-bb1e-a18160353ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分配运算资源\n",
    "adj = adj.to(device)\n",
    "features = features.to(device)\n",
    "labels = labels.to(device)\n",
    "idx_train = idx_train.to(device)\n",
    "idx_val = idx_val.to(device)\n",
    "idx_test = idx_test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa0550fd-d098-4c22-971c-fe661d219d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.6 定义Mish激活函数与图注意力层类\n",
    "def mish(x): # 性能优于RElu函数\n",
    "    return x * (torch.tanh(F.softplus(x)))\n",
    "# 图注意力层类\n",
    "class GraphAttentionLayer(nn.Module): # 图注意力层\n",
    "    # 初始化\n",
    "    def __init__(self,in_features,out_features,dropout=0.6):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.in_features = in_features # 定义输入特征维度\n",
    "        self.out_features = out_features # 定义输出特征维度\n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features,out_features)))\n",
    "        nn.init.xavier_uniform_(self.W) # 初始化全连接权重\n",
    "        self.a = nn.Parameter(torch.zeros(size=(2 * out_features,1)))\n",
    "        nn.init.xavier_uniform_(self.a) # 初始化注意力权重\n",
    " \n",
    "    def forward(self,input,adj):\n",
    "        h = torch.mm(input,self.W) # 全连接处理\n",
    "        N = h.size()[0]\n",
    "        # 对全连接后的特征数据分别进行基于批次维度和特征维度的复制，并将复制结果连接在一起。\n",
    "        # 这种操作使得顶点中的特征数据进行了充分的排列组合，结果中的每行信息都包含两个顶点特征。接下来的注意力机制便是基于每对顶点特征进行计算的。\n",
    "        a_input = torch.cat([h.repeat(1,N).view(N * N ,-1),h.repeat(N,1)],dim=1).view(N,-1,2 * self.out_features) # 主要功能将顶点特征两两搭配，连接在一起，生成数据形状[N,N,2 * self.out_features]\n",
    "        e = mish(torch.matmul(a_input,self.a).squeeze(2)) # 计算注意力\n",
    " \n",
    "        zero_vec = -9e15 * torch.ones_like(e) # 初始化最小值：该值用于填充被过滤掉的特征对象atenion。如果在过滤时，直接对过滤排的特征赋值为0，那么模型会无法收敛。\n",
    "        attention = torch.where(adj>0,e,zero_vec) # 过滤注意力 ：按照邻接矩阵中大于0的边对注意力结果进行过滤，使注意力按照图中的顶点配对的范围进行计算。\n",
    "        attention = F.softmax(attention,dim=1) # 对注意力分数进行归一化：使用F.Sofmax()函数对最终的注意力机制进行归一化，得到注意力分数(总和为1)。\n",
    "        attention = F.dropout(attention,self.dropout,training=self.training)\n",
    "        h_prime = torch.matmul(attention,h) # 使用注意力处理特征：将最终的注意力作用到全连接的结果上以完成计算。\n",
    "        return mish(h_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e92074e8-0478-4311-8ad3-82880f3bd03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 1433\n"
     ]
    }
   ],
   "source": [
    "# 1.7 搭建图注意力模型\n",
    "class GAT(nn.Module):# 图注意力模型类\n",
    "    def __init__(self,nfeat,nclasses,nhid,dropout,nheads): # 图注意力模型类的初始化方法，支持多套注意力机制同时运算，其参数nheads用于指定注意力的计算套数。\n",
    "        super(GAT, self).__init__()\n",
    "        # 注意力层\n",
    "        self.attentions = [GraphAttentionLayer(nfeat,nhid,dropout) for _ in range(nheads)] # 按照指定的注意力套数生成多套注意力层\n",
    "        for i , attention in enumerate(self.attentions): # 将注意力层添加到模型\n",
    "            self.add_module('attention_{}'.format(i),attention)\n",
    "        # 输出层\n",
    "        self.out_att = GraphAttentionLayer(nhid * nheads,nclasses,dropout)\n",
    " \n",
    "    def forward(self,x,adj): # 定义正向传播方法\n",
    "        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n",
    "        return self.out_att(x, adj)\n",
    " \n",
    " \n",
    "n_labels = labels.max().item() + 1 # 获取分类个数7\n",
    "n_features = features.shape[1] # 获取节点特征维度 1433\n",
    "print(n_labels,n_features) # 输出7与1433\n",
    " \n",
    "def accuracy(output,y): # 定义函数计算准确率\n",
    "    return (output.argmax(1) == y).type(torch.float32).mean().item()\n",
    " \n",
    "### 定义函数来实现模型的训练过程。与深度学习任务不同，图卷积在训练时需要传入样本间的关系数据。\n",
    "# 因为该关系数据是与节点数相等的方阵，所以传入的样本数也要与节点数相同，在计算loss值时，可以通过索引从总的运算结果中取出训练集的结果。\n",
    "def step(): # 定义函数来训练模型 Tip：在图卷积任务中，无论是用模型进行预测还是训练，都需要将全部的图结构方阵输入\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features,adj) # 将全部数据载入模型，只用训练数据计算损失\n",
    "    loss = F.cross_entropy(output[idx_train],labels[idx_train])\n",
    "    acc = accuracy(output[idx_train],labels[idx_train]) # 计算准确率\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item(),acc\n",
    " \n",
    "def evaluate(idx): # 定义函数来评估模型 Tip：在图卷积任务中，无论是用模型进行预测还是训练，都需要将全部的图结构方阵输入\n",
    "    model.eval()\n",
    "    output = model(features, adj) # 将全部数据载入模型，用指定索引评估模型结果\n",
    "    loss = F.cross_entropy(output[idx], labels[idx]).item()\n",
    "    return loss, accuracy(output[idx], labels[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f99b3040-98ac-46b2-a727-0c428b539300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "import itertools as it\n",
    "\n",
    "class Ranger(Optimizer):\n",
    " \n",
    "    def __init__(self, params, lr=1e-3, alpha=0.5, k=6, N_sma_threshhold=5, betas=(.95,0.999), eps=1e-5, weight_decay=0):\n",
    "        #parameter checks\n",
    "        if not 0.0 <= alpha <= 1.0:\n",
    "            raise ValueError(f'Invalid slow update rate: {alpha}')\n",
    "        if not 1 <= k:\n",
    "            raise ValueError(f'Invalid lookahead steps: {k}')\n",
    "        if not lr > 0:\n",
    "            raise ValueError(f'Invalid Learning Rate: {lr}')\n",
    "        if not eps > 0:\n",
    "            raise ValueError(f'Invalid eps: {eps}')\n",
    " \n",
    "        #parameter comments:\n",
    "        # beta1 (momentum) of .95 seems to work better than .90...\n",
    "        #N_sma_threshold of 5 seems better in testing than 4.\n",
    "        #In both cases, worth testing on your dataset (.90 vs .95, 4 vs 5) to make sure which works best for you.\n",
    " \n",
    "        #prep defaults and init torch.optim base\n",
    "        defaults = dict(lr=lr, alpha=alpha, k=k, step_counter=0, betas=betas, N_sma_threshhold=N_sma_threshhold, eps=eps, weight_decay=weight_decay)\n",
    "        super().__init__(params,defaults)\n",
    " \n",
    "        #adjustable threshold\n",
    "        self.N_sma_threshhold = N_sma_threshhold\n",
    " \n",
    "        #now we can get to work...\n",
    "        #removed as we now use step from RAdam...no need for duplicate step counting\n",
    "        #for group in self.param_groups:\n",
    "        #    group[\"step_counter\"] = 0\n",
    "            #print(\"group step counter init\")\n",
    " \n",
    "        #look ahead params\n",
    "        self.alpha = alpha\n",
    "        self.k = k \n",
    " \n",
    "        #radam buffer for state\n",
    "        self.radam_buffer = [[None,None,None] for ind in range(10)]\n",
    " \n",
    "        #self.first_run_check=0\n",
    " \n",
    "        #lookahead weights\n",
    "        #9/2/19 - lookahead param tensors have been moved to state storage.  \n",
    "        #This should resolve issues with load/save where weights were left in GPU memory from first load, slowing down future runs.\n",
    " \n",
    "        #self.slow_weights = [[p.clone().detach() for p in group['params']]\n",
    "        #                     for group in self.param_groups]\n",
    " \n",
    "        #don't use grad for lookahead weights\n",
    "        #for w in it.chain(*self.slow_weights):\n",
    "        #    w.requires_grad = False\n",
    " \n",
    "    def __setstate__(self, state):\n",
    "        print(\"set state called\")\n",
    "        super(Ranger, self).__setstate__(state)\n",
    " \n",
    " \n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        #note - below is commented out b/c I have other work that passes back the loss as a float, and thus not a callable closure.  \n",
    "        #Uncomment if you need to use the actual closure...\n",
    " \n",
    "        #if closure is not None:\n",
    "            #loss = closure()\n",
    " \n",
    "        #Evaluate averages and grad, update param tensors\n",
    "        for group in self.param_groups:\n",
    " \n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('Ranger optimizer does not support sparse gradients')\n",
    " \n",
    "                p_data_fp32 = p.data.float()\n",
    " \n",
    "                state = self.state[p]  #get state dict for this param\n",
    " \n",
    "                if len(state) == 0:   #if first time to run...init dictionary with our desired entries\n",
    "                    #if self.first_run_check==0:\n",
    "                        #self.first_run_check=1\n",
    "                        #print(\"Initializing slow buffer...should not see this at load from saved model!\")\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    " \n",
    "                    #look ahead weight storage now in state dict \n",
    "                    state['slow_buffer'] = torch.empty_like(p.data)\n",
    "                    state['slow_buffer'].copy_(p.data)\n",
    " \n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
    " \n",
    "                #begin computations \n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    " \n",
    "                #compute variance mov avg\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                #compute mean moving avg\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    " \n",
    "                state['step'] += 1\n",
    " \n",
    " \n",
    "                buffered = self.radam_buffer[int(state['step'] % 10)]\n",
    "                if state['step'] == buffered[0]:\n",
    "                    N_sma, step_size = buffered[1], buffered[2]\n",
    "                else:\n",
    "                    buffered[0] = state['step']\n",
    "                    beta2_t = beta2 ** state['step']\n",
    "                    N_sma_max = 2 / (1 - beta2) - 1\n",
    "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
    "                    buffered[1] = N_sma\n",
    "                    if N_sma > self.N_sma_threshhold:\n",
    "                        step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
    "                    else:\n",
    "                        step_size = 1.0 / (1 - beta1 ** state['step'])\n",
    "                    buffered[2] = step_size\n",
    " \n",
    "                if group['weight_decay'] != 0:\n",
    "                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
    " \n",
    "                if N_sma > self.N_sma_threshhold:\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    p_data_fp32.addcdiv_(-step_size * group['lr'], exp_avg, denom)\n",
    "                else:\n",
    "                    p_data_fp32.add_(-step_size * group['lr'], exp_avg)\n",
    " \n",
    "                p.data.copy_(p_data_fp32)\n",
    " \n",
    "                #integrated look ahead...\n",
    "                #we do it at the param level instead of group level\n",
    "                if state['step'] % group['k'] == 0:\n",
    "                    slow_p = state['slow_buffer'] #get access to slow param tensor\n",
    "                    slow_p.add_(self.alpha, p.data - slow_p)  #(fast weights - slow weights) * alpha\n",
    "                    p.data.copy_(slow_p)  #copy interpolated weights to RAdam param tensor\n",
    " \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84471638-7747-49a2-8fdd-bc475bc44a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/1000 [00:00<?, ?it/s]/var/folders/s1/1jpfx0m52rj4k7cgqkh7g3q40000gn/T/ipykernel_37912/2405757854.py:105: UserWarning: This overload of addcmul_ is deprecated:\n",
      "\taddcmul_(Number value, Tensor tensor1, Tensor tensor2)\n",
      "Consider using one of the following signatures instead:\n",
      "\taddcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1630.)\n",
      "  exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
      "  0%|                                        | 1/1000 [00:05<1:30:34,  5.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1/1000: train_loss=1.9358, train_acc=0.2250, val_loss=1.9374, val_acc=0.1967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▉                                        | 23/1000 [00:47<33:55,  2.08s/it]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1.8 使用Ranger优化器训练模型并可视化\n",
    "model = GAT(n_features, n_labels, 16,0.1,8).to(device) # 向GAT传入的后3个参数分别代表输出维度(16）、Dropout的丢弃率（0.1）、注意力的计算套数（8）\n",
    " \n",
    "from tqdm import tqdm\n",
    "# from Cora_ranger import * # 引入Ranger优化器\n",
    "optimizer = Ranger(model.parameters()) # 使用Ranger优化器\n",
    " \n",
    "# 训练模型\n",
    "epochs = 1000\n",
    "print_steps = 50\n",
    "train_loss, train_acc = [], []\n",
    "val_loss, val_acc = [], []\n",
    "for i in tqdm(range(epochs)):\n",
    "    tl,ta = step()\n",
    "    train_loss = train_loss + [tl]\n",
    "    train_acc = train_acc + [ta]\n",
    "    if (i+1) % print_steps == 0 or i == 0:\n",
    "        tl,ta = evaluate(idx_train)\n",
    "        vl,va = evaluate(idx_val)\n",
    "        val_loss = val_loss + [vl]\n",
    "        val_acc = val_acc + [va]\n",
    "        print(f'{i + 1:6d}/{epochs}: train_loss={tl:.4f}, train_acc={ta:.4f}' + f', val_loss={vl:.4f}, val_acc={va:.4f}')\n",
    " \n",
    "# 输出最终结果\n",
    "final_train, final_val, final_test = evaluate(idx_train), evaluate(idx_val), evaluate(idx_test)\n",
    "print(f'Train     : loss={final_train[0]:.4f}, accuracy={final_train[1]:.4f}')\n",
    "print(f'Validation: loss={final_val[0]:.4f}, accuracy={final_val[1]:.4f}')\n",
    "print(f'Test      : loss={final_test[0]:.4f}, accuracy={final_test[1]:.4f}')\n",
    " \n",
    "# 可视化训练过程\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15,5))\n",
    "ax = axes[0]\n",
    "axes[0].plot(train_loss[::print_steps] + [train_loss[-1]], label='Train')\n",
    "axes[0].plot(val_loss, label='Validation')\n",
    "axes[1].plot(train_acc[::print_steps] + [train_acc[-1]], label='Train')\n",
    "axes[1].plot(val_acc, label='Validation')\n",
    "for ax,t in zip(axes, ['Loss', 'Accuracy']): ax.legend(), ax.set_title(t, size=15)\n",
    " \n",
    "# 输出模型的预测结果\n",
    "output = model(features, adj)\n",
    "samples = 10\n",
    "idx_sample = idx_test[torch.randperm(len(idx_test))[:samples]]\n",
    "# 将样本标签与预测结果进行比较\n",
    "idx2lbl = {v:k for k,v in lbl2idx.items()}\n",
    "df = pd.DataFrame({'Real': [idx2lbl[e] for e in labels[idx_sample].tolist()],'Pred': [idx2lbl[e] for e in output[idx_sample].argmax(1).tolist()]})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6d06d1-c2ae-409f-8fcb-e01a2343008c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
