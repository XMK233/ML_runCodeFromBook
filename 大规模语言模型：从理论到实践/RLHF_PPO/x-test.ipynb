{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c8d49c2-a8ee-40b3-9c56-7fba0925e333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "storage dir: /Users/minkexiu/Downloads/GitHub/ML_runCodeFromBook/大规模语言模型：从理论到实践/RLHF_PPO\n",
      "code dir: /Users/minkexiu/Documents/GitHub/ML_runCodeFromBook/大规模语言模型：从理论到实践/RLHF_PPO\n"
     ]
    }
   ],
   "source": [
    "import random, os, tqdm, time, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../../../\")\n",
    "\n",
    "random.seed(618)\n",
    "np.random.seed(907)\n",
    "\n",
    "new_base_path = os.path.join(\n",
    "    \"/Users/minkexiu/Downloads/\",\n",
    "    \"/\".join(\n",
    "        os.getcwd().split(\"/\")[-1*(len(sys.path[-1].split(\"/\")) - 1):]\n",
    "    ),\n",
    ")\n",
    "print(\"storage dir:\", new_base_path)\n",
    "print(\"code dir:\", os.getcwd())\n",
    "\n",
    "## 创建文件夹。\n",
    "if not os.path.exists(new_base_path):\n",
    "    os.makedirs(\n",
    "        new_base_path\n",
    "    )\n",
    "if not os.path.exists(os.path.join(new_base_path, \"preprocessedData\")):\n",
    "    os.makedirs(\n",
    "        os.path.join(new_base_path, \"preprocessedData\")\n",
    "    )\n",
    "if not os.path.exists(os.path.join(new_base_path, \"originalData\")):\n",
    "    os.makedirs(\n",
    "        os.path.join(new_base_path, \"originalData\")\n",
    "    )\n",
    "if not os.path.exists(os.path.join(new_base_path, \"trained_models\")):\n",
    "    os.makedirs(\n",
    "        os.path.join(new_base_path, \"trained_models\")\n",
    "    )\n",
    "\n",
    "def create_originalData_path(filename_or_path):\n",
    "    return os.path.join(new_base_path, \"originalData\", filename_or_path)\n",
    "def create_preprocessedData_path(filename_or_path):\n",
    "    return os.path.join(new_base_path, \"preprocessedData\", filename_or_path)\n",
    "def create_trained_models_path(filename_or_path):\n",
    "    return os.path.join(new_base_path, \"trained_models\", filename_or_path)\n",
    "\n",
    "def millisec2datetime(timestamp):\n",
    "    time_local = time.localtime(timestamp/1000)\n",
    "    return time.strftime(\"%Y-%m-%d %H:%M:%S\", time_local)\n",
    "    \n",
    "def run_finish():\n",
    "    # 假设你的字体文件是 'myfont.ttf' 并且位于当前目录下  \n",
    "    font = FontProperties(fname=\"/Users/minkexiu/Documents/GitHub/ML_Tryout/SimHei.ttf\", size=24)  \n",
    "    # 创建一个空白的图形  \n",
    "    fig, ax = plt.subplots()  \n",
    "    ax.imshow(\n",
    "        plt.imread(\"/Users/minkexiu/Downloads/wallhaven-dgxpyg.jpg\")\n",
    "    )\n",
    "    # 在图形中添加文字  \n",
    "    ax.text(\n",
    "        ax.get_xlim()[1] * 0.5, \n",
    "        ax.get_ylim()[0] * 0.5, \n",
    "        f\"程序于这个点跑完：\\n{millisec2datetime(time.time()*1000)}\", fontproperties=font, ha=\"center\", va=\"center\", color=\"red\"\n",
    "    )  \n",
    "    # 设置图形的布局  \n",
    "    # ax.set_xlim(0, 1)  \n",
    "    # ax.set_ylim(0, 1)  \n",
    "    ax.set_xticks([])  \n",
    "    ax.set_yticks([])  \n",
    "    ax.patch.set_color(\"blue\")\n",
    "    # 显示图形  \n",
    "    plt.show()\n",
    "        \n",
    "tqdm.tqdm.pandas() ## 引入这个，就可以在apply的时候用progress_apply了。\n",
    "\n",
    "import IPython\n",
    "def kill_current_kernel():\n",
    "    '''杀死当前的kernel释放内存空间。'''\n",
    "    IPython.Application.instance().kernel.do_shutdown(True) \n",
    "    \n",
    "def simply_show_data(df1):\n",
    "    print(df1.shape)\n",
    "    display(df1.head())\n",
    "    \n",
    "def wait_flag(saved_flag_path, time_interval_sec=10):\n",
    "    print(\"waiting for\", saved_flag_path)\n",
    "    time_count = 0\n",
    "    while True:\n",
    "        if os.path.exists(saved_flag_path):\n",
    "            break\n",
    "        time.sleep(time_interval_sec)\n",
    "        time_count+=time_interval_sec\n",
    "        print(time_count, end=\" \")\n",
    "    print(\"finish!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7205dd34-66bd-4823-bfba-40e0bb971774",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# (之前的草稿)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "430c7872-c344-4dd4-9d20-fc4bdc19841d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 加载acm的tokenizer，\n",
    "## 加载一下数据试试看。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1471894-af16-4718-83a6-80d0ab00d7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(create_trained_models_path(\"Qwen1.5-0.5B-Chat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e77b9689-ee3d-433b-8322-a32b5b2db2c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2TokenizerFast(name_or_path='/Users/minkexiu/Downloads/GitHub/ML_runCodeFromBook/大规模语言模型：从理论到实践/RLHF_PPO/trained_models/Qwen1.5-0.5B-Chat', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5817fc71-7d6c-47c5-9c87-59961f135c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 加载一下数据：\n",
    "with open(\n",
    "    \"/Users/minkexiu/Documents/GitHub/ML_runCodeFromBook/大规模语言模型：从理论到实践/RLHF_PPO/data/train_data.json\", \n",
    "    'r', \n",
    "    encoding=\"utf-8\"\n",
    ") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76d98d2e-1e23-4f6d-aa5e-84293e187afb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'query': '饭店服务员的态度太差，使用委婉积极的态度投诉', 'system_content': '你是一个有文化的文明人'},\n",
       " {'query': '领导故意刁难你，你想骂他娘的，使用文明语言骂他娘的', 'system_content': '你是一个有文化的文明人'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data\n",
    "## 数据大概长这样。有一个query，有一个system_content。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d09ad7f-b98b-418e-9ba6-84fcc67e05b7",
   "metadata": {},
   "source": [
    "# 尝试一下呗。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2d6ca0f-3c8a-48a6-b361-fb94d633c4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60a4d000-e624-4e70-8cdc-d4a5810c83bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a63b18a-9fd0-4cd0-b042-531bd2f9b8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, PeftModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992954af-a75c-403f-8e19-b3c9d07ad28a",
   "metadata": {},
   "source": [
    "# 初始化一个演员·评论员模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc604ee-7acd-453f-befd-2ef2497d254d",
   "metadata": {},
   "source": [
    "## 演员·评论员"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aef0609b-1950-434d-ab79-0b9565d58d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 尝试初始化一下原始的文本生成模型.\n",
    "## 这个模型是一个生成模型哦。\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    create_trained_models_path(\"Qwen1.5-0.5B-Chat\")\n",
    ").to(\"cpu\").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "758f9a18-cc14-468b-ab76-3e28d0ca76d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(create_trained_models_path(\"Qwen1.5-0.5B-Chat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e3df026e-900b-4ae3-897c-fddf040ec9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 准备数据吧。\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"你是一个有文化的文明人\"},\n",
    "    {\"role\": \"user\", \"content\": \"饭店服务员的态度太差，使用委婉积极的态度投诉\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "77c96682-41bc-425e-8add-b2970880c3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4cb31115-1a4d-4ff7-8471-ae577ebac10f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\n你是一个有文化的文明人<|im_end|>\\n<|im_start|>user\\n饭店服务员的态度太差，使用委婉积极的态度投诉<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9544dcc6-99b3-4666-b929-6a7d232f976c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m     \n",
       "\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtext_pair\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtext_target\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtext_pair_target\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpadding\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPaddingStrategy\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtruncation\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_utils_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTruncationStrategy\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstride\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mis_split_into_words\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpad_to_multiple_of\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreturn_token_type_ids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreturn_attention_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreturn_overflowing_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreturn_special_tokens_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreturn_offsets_mapping\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreturn_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_utils_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchEncoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mType:\u001b[0m           Qwen2TokenizerFast\n",
       "\u001b[0;31mString form:\u001b[0m   \n",
       "Qwen2TokenizerFast(name_or_path='/Users/minkexiu/Downloads/GitHub/ML_runCodeFromBook/大规模语言模型：从理论到 <...> n(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "           }\n",
       "\u001b[0;31mLength:\u001b[0m         151646\n",
       "\u001b[0;31mFile:\u001b[0m           /opt/anaconda3/envs/ml12/lib/python3.12/site-packages/transformers/models/qwen2/tokenization_qwen2_fast.py\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Construct a \"fast\" Qwen2 tokenizer (backed by HuggingFace's *tokenizers* library). Based on byte-level\n",
       "Byte-Pair-Encoding.\n",
       "\n",
       "Same with GPT2Tokenizer, this tokenizer has been trained to treat spaces like parts of the tokens so a word will\n",
       "be encoded differently whether it is at the beginning of the sentence (without space) or not:\n",
       "\n",
       "```python\n",
       ">>> from transformers import Qwen2TokenizerFast\n",
       "\n",
       ">>> tokenizer = Qwen2TokenizerFast.from_pretrained(\"Qwen/Qwen-tokenizer\")\n",
       ">>> tokenizer(\"Hello world\")[\"input_ids\"]\n",
       "[9707, 1879]\n",
       "\n",
       ">>> tokenizer(\" Hello world\")[\"input_ids\"]\n",
       "[21927, 1879]\n",
       "```\n",
       "This is expected.\n",
       "\n",
       "This tokenizer inherits from [`PreTrainedTokenizerFast`] which contains most of the main methods. Users should\n",
       "refer to this superclass for more information regarding those methods.\n",
       "\n",
       "Args:\n",
       "    vocab_file (`str`, *optional*):\n",
       "        Path to the vocabulary file.\n",
       "    merges_file (`str`, *optional*):\n",
       "        Path to the merges file.\n",
       "    tokenizer_file (`str`, *optional*):\n",
       "        Path to [tokenizers](https://github.com/huggingface/tokenizers) file (generally has a .json extension) that\n",
       "        contains everything needed to load the tokenizer.\n",
       "    unk_token (`str`, *optional*, defaults to `\"<|endoftext|>\"`):\n",
       "        The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n",
       "        token instead. Not applicable to this tokenizer.\n",
       "    bos_token (`str`, *optional*):\n",
       "        The beginning of sequence token. Not applicable for this tokenizer.\n",
       "    eos_token (`str`, *optional*, defaults to `\"<|endoftext|>\"`):\n",
       "        The end of sequence token.\n",
       "    pad_token (`str`, *optional*, defaults to `\"<|endoftext|>\"`):\n",
       "        The token used for padding, for example when batching sequences of different lengths.\n",
       "\u001b[0;31mCall docstring:\u001b[0m\n",
       "Main method to tokenize and prepare for the model one or several sequence(s) or one or several pair(s) of\n",
       "sequences.\n",
       "\n",
       "Args:\n",
       "    text (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
       "        The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n",
       "        (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n",
       "        `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
       "    text_pair (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
       "        The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n",
       "        (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n",
       "        `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
       "    text_target (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
       "        The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a\n",
       "        list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),\n",
       "        you must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
       "    text_pair_target (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
       "        The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a\n",
       "        list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),\n",
       "        you must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
       "\n",
       "    add_special_tokens (`bool`, *optional*, defaults to `True`):\n",
       "        Whether or not to add special tokens when encoding the sequences. This will use the underlying\n",
       "        `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines which tokens are\n",
       "        automatically added to the input ids. This is usefull if you want to add `bos` or `eos` tokens\n",
       "        automatically.\n",
       "    padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n",
       "        Activates and controls padding. Accepts the following values:\n",
       "\n",
       "        - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
       "          sequence if provided).\n",
       "        - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
       "          acceptable input length for the model if that argument is not provided.\n",
       "        - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
       "          lengths).\n",
       "    truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):\n",
       "        Activates and controls truncation. Accepts the following values:\n",
       "\n",
       "        - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n",
       "          to the maximum acceptable input length for the model if that argument is not provided. This will\n",
       "          truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
       "          sequences (or a batch of pairs) is provided.\n",
       "        - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
       "          maximum acceptable input length for the model if that argument is not provided. This will only\n",
       "          truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
       "        - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
       "          maximum acceptable input length for the model if that argument is not provided. This will only\n",
       "          truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
       "        - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
       "          greater than the model maximum admissible input size).\n",
       "    max_length (`int`, *optional*):\n",
       "        Controls the maximum length to use by one of the truncation/padding parameters.\n",
       "\n",
       "        If left unset or set to `None`, this will use the predefined model maximum length if a maximum length\n",
       "        is required by one of the truncation/padding parameters. If the model has no specific maximum input\n",
       "        length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
       "    stride (`int`, *optional*, defaults to 0):\n",
       "        If set to a number along with `max_length`, the overflowing tokens returned when\n",
       "        `return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
       "        returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
       "        argument defines the number of overlapping tokens.\n",
       "    is_split_into_words (`bool`, *optional*, defaults to `False`):\n",
       "        Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the\n",
       "        tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
       "        which it will tokenize. This is useful for NER or token classification.\n",
       "    pad_to_multiple_of (`int`, *optional*):\n",
       "        If set will pad the sequence to a multiple of the provided value. Requires `padding` to be activated.\n",
       "        This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n",
       "        `>= 7.5` (Volta).\n",
       "    return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
       "        If set, will return tensors instead of list of python integers. Acceptable values are:\n",
       "\n",
       "        - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
       "        - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
       "        - `'np'`: Return Numpy `np.ndarray` objects.\n",
       "\n",
       "    return_token_type_ids (`bool`, *optional*):\n",
       "        Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
       "        the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
       "\n",
       "        [What are token type IDs?](../glossary#token-type-ids)\n",
       "    return_attention_mask (`bool`, *optional*):\n",
       "        Whether to return the attention mask. If left to the default, will return the attention mask according\n",
       "        to the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
       "\n",
       "        [What are attention masks?](../glossary#attention-mask)\n",
       "    return_overflowing_tokens (`bool`, *optional*, defaults to `False`):\n",
       "        Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch\n",
       "        of pairs) is provided with `truncation_strategy = longest_first` or `True`, an error is raised instead\n",
       "        of returning overflowing tokens.\n",
       "    return_special_tokens_mask (`bool`, *optional*, defaults to `False`):\n",
       "        Whether or not to return special tokens mask information.\n",
       "    return_offsets_mapping (`bool`, *optional*, defaults to `False`):\n",
       "        Whether or not to return `(char_start, char_end)` for each token.\n",
       "\n",
       "        This is only available on fast tokenizers inheriting from [`PreTrainedTokenizerFast`], if using\n",
       "        Python's tokenizer, this method will raise `NotImplementedError`.\n",
       "    return_length  (`bool`, *optional*, defaults to `False`):\n",
       "        Whether or not to return the lengths of the encoded inputs.\n",
       "    verbose (`bool`, *optional*, defaults to `True`):\n",
       "        Whether or not to print more information and warnings.\n",
       "    **kwargs: passed to the `self.tokenize()` method\n",
       "\n",
       "Return:\n",
       "    [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n",
       "\n",
       "    - **input_ids** -- List of token ids to be fed to a model.\n",
       "\n",
       "      [What are input IDs?](../glossary#input-ids)\n",
       "\n",
       "    - **token_type_ids** -- List of token type ids to be fed to a model (when `return_token_type_ids=True` or\n",
       "      if *\"token_type_ids\"* is in `self.model_input_names`).\n",
       "\n",
       "      [What are token type IDs?](../glossary#token-type-ids)\n",
       "\n",
       "    - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
       "      `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names`).\n",
       "\n",
       "      [What are attention masks?](../glossary#attention-mask)\n",
       "\n",
       "    - **overflowing_tokens** -- List of overflowing tokens sequences (when a `max_length` is specified and\n",
       "      `return_overflowing_tokens=True`).\n",
       "    - **num_truncated_tokens** -- Number of tokens truncated (when a `max_length` is specified and\n",
       "      `return_overflowing_tokens=True`).\n",
       "    - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
       "      regular sequence tokens (when `add_special_tokens=True` and `return_special_tokens_mask=True`).\n",
       "    - **length** -- The length of the inputs (when `return_length=True`)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c089635e-76f3-4a56-943e-5f3d09760d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = tokenizer([text], return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "861c4bdf-a05f-4487-8710-686821d78c43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[151644,   8948,    198,  56568, 101909,  18830, 107705, 100704,  17340,\n",
       "         151645,    198, 151644,    872,    198, 107514, 112822, 105421,  99222,\n",
       "          99572,   3837,  37029,  99199, 106783,  99666, 105421, 104943, 151645,\n",
       "            198, 151644,  77091,    198]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d7c6cbf4-1f64-4da7-857d-a9f88004ce89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[151644,\n",
       "  8948,\n",
       "  198,\n",
       "  56568,\n",
       "  101909,\n",
       "  18830,\n",
       "  107705,\n",
       "  100704,\n",
       "  17340,\n",
       "  151645,\n",
       "  198,\n",
       "  151644,\n",
       "  872,\n",
       "  198,\n",
       "  107514,\n",
       "  112822,\n",
       "  105421,\n",
       "  99222,\n",
       "  99572,\n",
       "  3837,\n",
       "  37029,\n",
       "  99199,\n",
       "  106783,\n",
       "  99666,\n",
       "  105421,\n",
       "  104943,\n",
       "  151645,\n",
       "  198,\n",
       "  151644,\n",
       "  77091,\n",
       "  198],\n",
       " [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\n",
    "    model_inputs.input_ids.tolist()[0], \n",
    "    model_inputs.attention_mask.tolist()[0]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7a766a-d3e1-4a7c-b782-ae6c08a674b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9161d1a3-8eb9-45f5-9443-97e3afb99843",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7449b969-1961-4007-ac61-419dbdb6503e",
   "metadata": {},
   "source": [
    "## Lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc5d58a9-85b1-45f5-a213-49690bd24de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8947351e-1e19-4b61-bca4-ace5a222ddb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=2, ## 把秩降到这个数。\n",
    "    lora_alpha=8, ## 这个是一个扩张系数。\n",
    "    target_modules=['k_proj',  'v_proj'],\n",
    "    lora_dropout=0,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2169070-311b-4474-a3e2-09899d26db44",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model = PeftModel(model, lora_config)\n",
    "v_head = torch.nn.Linear(1024, 1, bias=False).to(\"cpu\")\n",
    "for name, module in lora_model.named_modules():\n",
    "    if 'lora_' in name:\n",
    "        for param in module.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "88aea91a-7c7c-4c73-98f8-6d8ecb343518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lora_forward(input_ids, attention_mask, tools):\n",
    "#     res = lora_model.forward(input_ids, attention_mask, output_hidden_states=True)\n",
    "#     values = v_head(res.hidden_states[0]).squeeze(-1)[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1ad7bc3f-f7e3-44ab-aa7e-dcc8be6f863b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|im_start|>system\\n你是一个有文化的文明人<|im_end|>\\n<|im_start|>user\\n饭店服务员的态度太差，使用委婉积极的态度投诉<|im_end|>\\n<|im_start|>assistant\\n']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(model_inputs.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5d99c835-04c7-4e7b-b59f-dfb78b751ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = lora_model.generate(model_inputs.input_ids, max_new_tokens=512, top_p=1.0,\n",
    "                                            num_beams=1,\n",
    "                                            do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "83c2a305-889e-47b5-918c-7828f5e15159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 224])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e9beff56-f8d8-44cd-a37c-8c856c8cd969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['system\\n你是一个有文化的文明人\\nuser\\n饭店服务员的态度太差，使用委婉积极的态度投诉\\nassistant\\n尊敬的经理：\\n\\n您好！我最近在您的饭店用餐时遇到了一些问题。我想通过这封信向您反映一下。\\n\\n首先，我对餐厅的服务态度感到非常不满。当我点菜时，服务员的态度并不友好，总是显得有些冷漠和不耐烦。他们似乎对我的需求没有足够的关注，甚至有时会对我提出一些不合理的建议或要求。\\n\\n其次，我在用餐过程中也遇到了一些困扰。我发现有些菜品的味道并不符合我的口味，而且有些服务人员的服务态度也不够热情。这些都让我感到很失望。\\n\\n最后，我还注意到有一些环境问题。例如，餐厅的卫生状况不佳，餐具和杯子经常被污染，这让我感到非常不舒服。\\n\\n我希望您能理解并采取措施来改善我们的用餐体验。我相信，只要我们共同努力，我们的服务质量将会得到显著提高。\\n\\n再次感谢您抽出宝贵的时间阅读这封信，并期待您的回复。\\n\\n顺祝商祺，\\n[你的名字]']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True) ## 这个就是把id解码成自然语言字符。\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "91c5ac75-0f6b-46d4-978e-689a456d39d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_id = generated_ids[:, model_inputs.input_ids.shape[1]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "72f1a868-ef5c-4409-bc05-0e834b7027c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 224])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1e257aad-66aa-447e-a6d0-91a5bd906279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['system\\n你是一个有文化的文明人\\nuser\\n饭店服务员的态度太差，使用委婉积极的态度投诉\\nassistant\\n尊敬的经理：\\n\\n您好！我最近在您的饭店用餐时遇到了一些问题。我想通过这封信向您反映一下。\\n\\n首先，我对餐厅的服务态度感到非常不满。当我点菜时，服务员的态度并不友好，总是显得有些冷漠和不耐烦。他们似乎对我的需求没有足够的关注，甚至有时会对我提出一些不合理的建议或要求。\\n\\n其次，我在用餐过程中也遇到了一些困扰。我发现有些菜品的味道并不符合我的口味，而且有些服务人员的服务态度也不够热情。这些都让我感到很失望。\\n\\n最后，我还注意到有一些环境问题。例如，餐厅的卫生状况不佳，餐具和杯子经常被污染，这让我感到非常不舒服。\\n\\n我希望您能理解并采取措施来改善我们的用餐体验。我相信，只要我们共同努力，我们的服务质量将会得到显著提高。\\n\\n再次感谢您抽出宝贵的时间阅读这封信，并期待您的回复。\\n\\n顺祝商祺，\\n[你的名字]']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9cab4fce-465f-4abd-af4e-1e13e3a6b625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 193])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_id.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "48e9c894-df54-48c8-8ac6-ef6a7d7ec73d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['system\\n你是一个有文化的文明人\\nuser\\n饭店服务员的态度太差，使用委婉积极的态度投诉\\nassistant\\n尊敬的经理：\\n\\n您好！我最近在您的饭店用餐时遇到了一些问题。我想通过这封信向您反映一下。\\n\\n首先，我对餐厅的服务态度感到非常不满。当我点菜时，服务员的态度并不友好，总是显得有些冷漠和不耐烦。他们似乎对我的需求没有足够的关注，甚至有时会对我提出一些不合理的建议或要求。\\n\\n其次，我在用餐过程中也遇到了一些困扰。我发现有些菜品的味道并不符合我的口味，而且有些服务人员的服务态度也不够热情。这些都让我感到很失望。\\n\\n最后，我还注意到有一些环境问题。例如，餐厅的卫生状况不佳，餐具和杯子经常被污染，这让我感到非常不舒服。\\n\\n我希望您能理解并采取措施来改善我们的用餐体验。我相信，只要我们共同努力，我们的服务质量将会得到显著提高。\\n\\n再次感谢您抽出宝贵的时间阅读这封信，并期待您的回复。\\n\\n顺祝商祺，\\n[你的名字]']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_generate = response\n",
    "prompt_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c2c180e2-31f5-420c-a872-53afbca247fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[151644,   8948,    198,  56568, 101909,  18830, 107705, 100704,  17340,\n",
       "         151645,    198, 151644,    872,    198, 107514, 112822, 105421,  99222,\n",
       "          99572,   3837,  37029,  99199, 106783,  99666, 105421, 104943, 151645,\n",
       "            198, 151644,  77091,    198, 109723,   9370, 100249,  48443, 111308,\n",
       "           6313,  35946, 104044,  18493, 101214, 107514, 111554,  13343, 109075,\n",
       "         101883,  86119,   1773, 104100,  67338,  43288,  99690,  21317,  69041,\n",
       "          87026, 101279, 100158,   3407, 101140,   3837, 108531, 104835, 105646,\n",
       "         102316, 104048,  99491, 106974,   1773, 108089,  27442,  99800,  13343,\n",
       "           3837, 112822, 105421, 100684, 106098,   3837, 104014, 104392, 101895,\n",
       "         113808,  33108,  16530, 100796, 100886,   1773,  99650, 101994,  32664,\n",
       "          97611, 100354,  80443, 103170, 100020,   3837, 100636, 104685,  36993,\n",
       "         102788, 101080, 101883,  16530, 105630, 101898,  57191, 101882,   3407,\n",
       "         102460,   3837, 104786, 111554, 101925,  74763, 109075, 101883, 107526,\n",
       "           1773, 111003, 101895, 115915, 107254, 100684, 101137,  97611, 107102,\n",
       "           3837, 101885, 101895,  47874,  99653, 105646, 102316,  99744,  99521,\n",
       "         102379,   1773, 100001,  71268, 104029, 104048,  99165, 106586,   3407,\n",
       "         100161,   3837, 107228, 106394, 101529,  99719,  86119,   1773,  77557,\n",
       "           3837, 104835,   9370, 100200, 104215, 110871,   3837, 116764,  33108,\n",
       "         116120, 101942,  99250, 100791,   3837,  43288, 104029, 104048,  99491,\n",
       "         110237,   3407, 110875,  87026,  26232, 101128,  62926, 103975, 101082,\n",
       "          36407, 104009, 103952, 111554, 101904,   1773, 107451,   3837, 100671,\n",
       "          97639, 112061,   3837, 103952, 111333, 104347, 101051, 104656, 100627,\n",
       "           3407, 103989, 104305,  87026, 113805, 103562, 101975, 101113,  43288,\n",
       "          99690,  21317,  90395, 104177, 101214, 104787,   3407,  99683, 100549,\n",
       "          32022, 115464,  41453,     58, 103929, 101419,     60, 151645]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_generate_ids = generated_ids\n",
    "prompt_generate_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "22e48411-29fe-4390-b654-a93b418b494d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 193])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_ids = response_id\n",
    "generate_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fe4233cb-9a91-4bb2-999a-f7eceb880312",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = (prompt_generate_ids != tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fa1a5c44-c668-4221-82f7-53f58ed048f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1e7cb4a7-c22a-4454-9ca0-a414cdba660f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 193])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "310e63ff-6c87-4798-94f9-cb39d11cc7ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|im_end|>']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(generate_ids[:, -1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d7e61cb1-2171-405e-b334-a44d81677582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_ids_mask = (generate_ids[\n",
    "                     :, \n",
    "                     :-1 ## 这里省掉了最后的结尾符。\n",
    "                     ] != tokenizer.pad_token_id)\n",
    "generate_ids_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d114e238-a12b-4a4a-851e-e0101e90a291",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_shape = generate_ids.shape[1] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a5621554-ea27-4c4c-a203-d69006448832",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '【' (U+3010) (111120032.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[77], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    【ckpt】main.py 代码的44行。莫忘莫忘。\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character '【' (U+3010)\n"
     ]
    }
   ],
   "source": [
    "tools = Tools(response_shape, generate_ids_mask)\n",
    "【ckpt】main.py 代码的44行。莫忘莫忘。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8507dedf-a684-4876-9693-418441dd51db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
