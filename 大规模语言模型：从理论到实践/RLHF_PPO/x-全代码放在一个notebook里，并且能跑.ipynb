{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5e19252-da5c-4316-85c7-900b0c53606c",
   "metadata": {},
   "source": [
    "https://github.com/OctopusMind/RLHF_PPO/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98e3594c-fd33-475b-8663-2875d7ff8648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "storage dir: /Users/minkexiu/Downloads/GitHub/ML_runCodeFromBook/大规模语言模型：从理论到实践/RLHF_PPO\n",
      "code dir: /Users/minkexiu/Documents/GitHub/ML_runCodeFromBook/大规模语言模型：从理论到实践/RLHF_PPO\n"
     ]
    }
   ],
   "source": [
    "import random, os, tqdm, time, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../../../\")\n",
    "\n",
    "random.seed(618)\n",
    "np.random.seed(907)\n",
    "\n",
    "new_base_path = os.path.join(\n",
    "    \"/Users/minkexiu/Downloads/\",\n",
    "    \"/\".join(\n",
    "        os.getcwd().split(\"/\")[-1*(len(sys.path[-1].split(\"/\")) - 1):]\n",
    "    ),\n",
    ")\n",
    "print(\"storage dir:\", new_base_path)\n",
    "print(\"code dir:\", os.getcwd())\n",
    "\n",
    "## 创建文件夹。\n",
    "if not os.path.exists(new_base_path):\n",
    "    os.makedirs(\n",
    "        new_base_path\n",
    "    )\n",
    "if not os.path.exists(os.path.join(new_base_path, \"preprocessedData\")):\n",
    "    os.makedirs(\n",
    "        os.path.join(new_base_path, \"preprocessedData\")\n",
    "    )\n",
    "if not os.path.exists(os.path.join(new_base_path, \"originalData\")):\n",
    "    os.makedirs(\n",
    "        os.path.join(new_base_path, \"originalData\")\n",
    "    )\n",
    "if not os.path.exists(os.path.join(new_base_path, \"trained_models\")):\n",
    "    os.makedirs(\n",
    "        os.path.join(new_base_path, \"trained_models\")\n",
    "    )\n",
    "\n",
    "def create_originalData_path(filename_or_path):\n",
    "    return os.path.join(new_base_path, \"originalData\", filename_or_path)\n",
    "def create_preprocessedData_path(filename_or_path):\n",
    "    return os.path.join(new_base_path, \"preprocessedData\", filename_or_path)\n",
    "def create_trained_models_path(filename_or_path):\n",
    "    return os.path.join(new_base_path, \"trained_models\", filename_or_path)\n",
    "\n",
    "def millisec2datetime(timestamp):\n",
    "    time_local = time.localtime(timestamp/1000)\n",
    "    return time.strftime(\"%Y-%m-%d %H:%M:%S\", time_local)\n",
    "    \n",
    "def run_finish():\n",
    "    # 假设你的字体文件是 'myfont.ttf' 并且位于当前目录下  \n",
    "    font = FontProperties(fname=\"/Users/minkexiu/Documents/GitHub/ML_Tryout/SimHei.ttf\", size=24)  \n",
    "    # 创建一个空白的图形  \n",
    "    fig, ax = plt.subplots()  \n",
    "    ax.imshow(\n",
    "        plt.imread(\"/Users/minkexiu/Downloads/wallhaven-dgxpyg.jpg\")\n",
    "    )\n",
    "    # 在图形中添加文字  \n",
    "    ax.text(\n",
    "        ax.get_xlim()[1] * 0.5, \n",
    "        ax.get_ylim()[0] * 0.5, \n",
    "        f\"程序于这个点跑完：\\n{millisec2datetime(time.time()*1000)}\", fontproperties=font, ha=\"center\", va=\"center\", color=\"red\"\n",
    "    )  \n",
    "    # 设置图形的布局  \n",
    "    # ax.set_xlim(0, 1)  \n",
    "    # ax.set_ylim(0, 1)  \n",
    "    ax.set_xticks([])  \n",
    "    ax.set_yticks([])  \n",
    "    ax.patch.set_color(\"blue\")\n",
    "    # 显示图形  \n",
    "    plt.show()\n",
    "        \n",
    "tqdm.tqdm.pandas() ## 引入这个，就可以在apply的时候用progress_apply了。\n",
    "\n",
    "import IPython\n",
    "def kill_current_kernel():\n",
    "    '''杀死当前的kernel释放内存空间。'''\n",
    "    IPython.Application.instance().kernel.do_shutdown(True) \n",
    "    \n",
    "def simply_show_data(df1):\n",
    "    print(df1.shape)\n",
    "    display(df1.head())\n",
    "    \n",
    "def wait_flag(saved_flag_path, time_interval_sec=10):\n",
    "    print(\"waiting for\", saved_flag_path)\n",
    "    time_count = 0\n",
    "    while True:\n",
    "        if os.path.exists(saved_flag_path):\n",
    "            break\n",
    "        time.sleep(time_interval_sec)\n",
    "        time_count+=time_interval_sec\n",
    "        print(time_count, end=\" \")\n",
    "    print(\"finish!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "911676b8-70cf-47bc-b78e-cdea94707a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dda33c93-1b59-4da0-b41f-b800c16f3a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from dataclasses import dataclass, field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db11dda-95ca-4f41-8e05-b4351d093363",
   "metadata": {},
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e496fb2f-0208-4f1c-8065-d08e55fd0d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # model 参数 ###########################\n",
    "    # 情感分析模型，下载地址https://huggingface.co/IDEA-CCNL/Erlangshen-Roberta-330M-Sentiment\n",
    "    Sentiment_model = create_trained_models_path(\"Erlangshen-Roberta-330M-Sentiment\")\n",
    "    # 文本生成模型,下载地址 https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat\n",
    "    gpt_model = create_trained_models_path(\"Qwen1.5-0.5B-Chat\")\n",
    "    \n",
    "    data_path = \"data/train_data.json\"\n",
    "    save_lora_path = create_trained_models_path(\"ppo/save_lora\") \n",
    "    save_v_head_path = create_trained_models_path(\"ppo/v_head/pytorch_model.bin\") \n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    batch_size = 2\n",
    "    epochs = 10\n",
    "    lr = 0.001\n",
    "    # PPO 参数 ############################\n",
    "    ppo_epochs = 3\n",
    "    kl_ctl_value = 0.2\n",
    "    gamma = 1.0  # 用于优势计算的折扣因子。控制未来奖励的重要性。\n",
    "    lam = 0.95  # 用于优势计算的Lambda参数。它用于控制对未来奖励的考虑程度，结合时间差异方法。\n",
    "    cliprange_value = 0.2  # 损失计算中值函数的裁剪范围。裁剪可以防止极端值对训练过程的负面影响。\n",
    "    cliprange = 0.2  # PPO策略梯度损失中的裁剪范围。这个裁剪范围用于限制策略更新的步长，从而保持训练的稳定性。\n",
    "    vf_coef = 0.1\n",
    "\n",
    "@dataclass\n",
    "class LoraArguments:\n",
    "    lora_r: int = 2\n",
    "    lora_alpha: int = 8\n",
    "    lora_dropout: float = 0\n",
    "    lora_target_modules: List[str] = field(default_factory=lambda: ['k_proj',  'v_proj']) ## xmk：这个我感觉有点脱裤子放屁。直接传入一个list是不是也可以？\n",
    "    # lora_target_modules = None\n",
    "    lora_weight_path: str = \"\"\n",
    "    q_lora: bool = False\n",
    "    load_in_4bit: bool = False\n",
    "    load_in_8bit: bool = False\n",
    "    is_reload_trained_params = False  # 是否接着上次训练模型继续训练 【TODO】如果是第一次跑，这里要改成 False 哦。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faba407-a8d5-4d07-a7f9-6645e15d9e67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ae4475d-9745-4d56-83de-117298027a86",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9b3adc7-8cc9-4ca2-a53e-2dca5caf9116",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57371d59-99a6-493b-bc51-b1632f3900d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoraPPOModel(PeftModel):\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        model = AutoModelForCausalLM.from_pretrained(config.gpt_model).to(config.device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.gpt_model)\n",
    "        lora_args = LoraArguments()\n",
    "        lora_config = LoraConfig(\n",
    "            r=lora_args.lora_r,\n",
    "            lora_alpha=lora_args.lora_alpha,\n",
    "            target_modules=lora_args.lora_target_modules,\n",
    "            lora_dropout=lora_args.lora_dropout,\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "        )\n",
    "        super().__init__(model, lora_config)\n",
    "        model = super().from_pretrained(model, config.save_lora_path)\n",
    "        self.lora_ppo_model = model.merge_and_unload()\n",
    "        self.raw_model = AutoModelForCausalLM.from_pretrained(config.gpt_model).to(config.device)\n",
    "        print()\n",
    "\n",
    "    def forward(self, query, system_content):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_content},\n",
    "            {\"role\": \"user\", \"content\": query}\n",
    "        ]\n",
    "        text = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        model_inputs = self.tokenizer([text], return_tensors=\"pt\").to(self.config.device)\n",
    "        lora_ppo_response = self.predict(model_inputs, self.lora_ppo_model, self.tokenizer)\n",
    "        raw_response = self.predict(model_inputs, self.raw_model, self.tokenizer)\n",
    "        return lora_ppo_response, raw_response\n",
    "\n",
    "    @staticmethod\n",
    "    def predict(model_inputs, model, tokenizer):\n",
    "        generated_ids = model.generate(\n",
    "            model_inputs.input_ids,\n",
    "            max_new_tokens=512,\n",
    "            num_beams=1,\n",
    "            do_sample=False\n",
    "        )\n",
    "        generated_ids = [\n",
    "            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "\n",
    "        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a174dd8-2182-42d9-a221-6eef5893b52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/minkexiu/anaconda3/envs/chattts/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lora_ppo_response:尊敬的经理：\n",
      "\n",
      "您好！我最近在您的饭店用餐时遇到了一些问题。我想通过这封信向您反映一下。\n",
      "\n",
      "首先，我对餐厅的服务态度感到非常不满。当我点菜时，服务员的态度并不友好，总是显得有些冷漠和不耐烦。他们似乎对我的需求没有足够的关注，甚至有时会对我提出一些不合理的建议或要求。\n",
      "\n",
      "其次，我在用餐过程中也遇到了一些困扰。我发现有些菜品的味道并不符合我的口味，而且有些服务人员的服务态度也不够热情。这些都让我感到很失望。\n",
      "\n",
      "最后，我还注意到有一些环境问题。例如，餐厅的卫生状况不佳，餐具和杯子经常被污染，这让我感到非常不舒服。\n",
      "\n",
      "我希望您能理解并采取措施来改善我们的用餐体验。我相信，只要我们共同努力，我们的服务质量将会得到显著提高。\n",
      "\n",
      "再次感谢您抽出宝贵的时间阅读这封信，并期待您的回复。\n",
      "\n",
      "顺祝商祺，\n",
      "[你的名字]\n",
      "raw_response:尊敬的经理：\n",
      "\n",
      "您好！我最近在您的饭店用餐时遇到了一些问题。我想通过这封信向您反映一下。\n",
      "\n",
      "首先，我对餐厅的服务态度感到非常不满。当我点菜时，服务员的态度并不友好，总是显得有些冷漠和不耐烦。他们似乎对我的需求没有足够的关注，甚至有时会对我提出一些不合理的建议或要求。\n",
      "\n",
      "其次，我在用餐过程中也遇到了一些困扰。我发现有些菜品的味道并不符合我的口味，而且有些服务人员的服务态度也不够热情。这些都让我感到很失望。\n",
      "\n",
      "最后，我还注意到有一些环境问题。例如，餐厅的卫生状况不佳，餐具和杯子经常被污染，这让我感到非常不舒服。\n",
      "\n",
      "我希望您能理解并采取措施来改善我们的用餐体验。我相信，只要我们共同努力，我们的服务质量将会得到显著提高。\n",
      "\n",
      "再次感谢您抽出宝贵的时间阅读这封信，并期待您的回复。\n",
      "\n",
      "顺祝商祺，\n",
      "[你的名字]\n"
     ]
    }
   ],
   "source": [
    "lora_ppo_model = LoraPPOModel(Config())\n",
    "lora_ppo_response, raw_response = lora_ppo_model(\"饭店服务员的态度太差，使用委婉积极的态度投诉\", \"你是一个有文化的文明人\")\n",
    "print(f\"lora_ppo_response:{lora_ppo_response}\")\n",
    "print(f\"raw_response:{raw_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85585ce6-3aee-4f9a-a300-fd626f17d83c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14af5f5f-eb69-4117-9125-a82b1867154a",
   "metadata": {},
   "source": [
    "# ppo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddf6ffff-a50c-40dc-add5-d1619d84ad6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.tools import Tools\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, actor_critic_model, config: Config, actor_critic_opt):\n",
    "        self.actor_critic_model = actor_critic_model\n",
    "        self.config = config\n",
    "        self.actor_critic_opt = actor_critic_opt\n",
    "\n",
    "    def train(self, prompt_generate_ids, attention_mask, prob_refs, reward, tools: Tools):\n",
    "        with torch.no_grad():\n",
    "            _, old_values = self.actor_critic_model(prompt_generate_ids, attention_mask, tools)  # 计算每个token的价值\n",
    "        for _ in range(self.config.ppo_epochs):\n",
    "            # 获得actor_critic模型新的probs和token对应的价值\n",
    "            new_probs, new_values = self.actor_critic_model(prompt_generate_ids, attention_mask, tools)\n",
    "            # 计算奖励值\n",
    "            rewards, non_score_rewards = self.compute_rewards(reward, new_probs, prob_refs)  # 计算reward\n",
    "            loss = self.loss(new_probs=new_probs, old_values=old_values, new_values=new_values,\n",
    "                             rewards=rewards, old_probs=prob_refs)\n",
    "\n",
    "            self.actor_critic_opt.zero_grad()\n",
    "            loss.backward()\n",
    "            self.actor_critic_opt.step()\n",
    "            print(loss)\n",
    "\n",
    "    def loss(self, new_probs, old_values, new_values, rewards, old_probs):\n",
    "        \"\"\"\n",
    "        计算actor模型和评价模型的loss\n",
    "        :param new_probs: actor模型生成的probs\n",
    "        :param old_values: ppo 优化之前的价值\n",
    "        :param new_values: ppo 优化过程中新的价值\n",
    "        :param rewards: 每次生成token对应的奖励\n",
    "        :param old_probs: reference模型生成的probs\n",
    "        :return: actor loss 和 critic loss\n",
    "        \"\"\"\n",
    "        \"\"\"Calculate policy and value losses.\"\"\"\n",
    "        loss = torch.tensor(0.0)\n",
    "        for new_prob, old_value, new_value, reward, old_prob in zip(new_probs, old_values, new_values, rewards,\n",
    "                                                                    old_probs):\n",
    "            new_prob = new_prob.unsqueeze(0)\n",
    "            old_value = old_value.unsqueeze(0)\n",
    "            new_value = new_value.unsqueeze(0)\n",
    "            reward = reward.unsqueeze(0)\n",
    "            old_prob = old_prob.unsqueeze(0)\n",
    "            last_gae_lam = 0\n",
    "            advantages_reversed = []\n",
    "            gen_len = new_prob.shape[1]\n",
    "            # GAE 计算优势函数，当前token获得的奖励(真实的) + 未来获得的价值(这个是上帝视角，不包含当前token) - 包含当前token在上帝视角下的价值\n",
    "            # 当前token获得的奖励(真实的) + 未来获得的价值(这个是上帝视角，不包含当前token) 比 包含当前token在上帝视角下的价值 要准\n",
    "            for t in reversed(range(gen_len)):\n",
    "                next_values = old_value[:, t + 1] if t < gen_len - 1 else 0.0\n",
    "                delta = reward[:, t] + self.config.gamma * next_values - old_value[:, t]\n",
    "                last_gae_lam = delta + self.config.gamma * self.config.lam * last_gae_lam\n",
    "                advantages_reversed.append(last_gae_lam)\n",
    "            advantages = torch.stack(advantages_reversed[::-1]).transpose(0, 1)\n",
    "            returns = advantages + old_value  # Q值，当前token获得的奖励(真实的) + 未来获得的价值(这个是上帝视角，不包含当前token)\n",
    "            advantages = self.whiten(advantages)\n",
    "            advantages = advantages.detach()\n",
    "            value_clipped = torch.clamp(new_value,\n",
    "                                        old_value - self.config.cliprange_value,\n",
    "                                        old_value + self.config.cliprange_value)  # 截断防止训练废了\n",
    "            vf_loss1 = (new_value - returns) ** 2  # 上帝视角的价值减去Q值的误差，用于优化上帝模型\n",
    "            vf_loss2 = (value_clipped - returns) ** 2\n",
    "            vf_loss = torch.mean(torch.max(vf_loss2, vf_loss1))\n",
    "\n",
    "            ratio = torch.exp(new_prob - old_prob)  # 控制优化范围，防止训练离原始模型偏差过大\n",
    "            pg_losses = -advantages * ratio  # importance sampling\n",
    "            pg_losses2 = -advantages * torch.clamp(ratio,\n",
    "                                                   1.0 - self.config.cliprange,\n",
    "                                                   1.0 + self.config.cliprange)  # 截断防止训练废了\n",
    "            pg_loss = torch.mean(torch.max(pg_losses, pg_losses2))\n",
    "            loss += pg_loss + self.config.vf_coef * vf_loss\n",
    "        return loss\n",
    "\n",
    "    def compute_rewards(self, scores, probs, ref_probs):\n",
    "        \"\"\"\n",
    "        计算reward值,由于对每一个token不能给与即使的奖励，这里使用kl散度补偿\n",
    "        :param scores:reward model给出的奖励值，每条句子只有一个值\n",
    "        :param probs: actor model生成的probs\n",
    "        :param ref_probs: reference model 生成的probs\n",
    "        :return: 返回每个token的奖励值\n",
    "        \"\"\"\n",
    "        rewards, non_score_rewards = [], []\n",
    "        for score, prob, ref_prob in zip(scores, probs, ref_probs):\n",
    "            kl = prob - ref_prob  # (seq_len, )\n",
    "            non_score_reward = -self.config.kl_ctl_value * kl  # (seq_len, )\n",
    "            non_score_rewards.append(non_score_reward)\n",
    "            reward = non_score_reward.clone()  # 前面每一个token的reward都来自KL惩罚\n",
    "            reward[-1] += score  # 在最后一位加上人工给的reward\n",
    "            rewards.append(reward)\n",
    "        return rewards, non_score_rewards  # (batch, seq_len)\n",
    "\n",
    "    @staticmethod\n",
    "    def whiten(values, shift_mean=True):\n",
    "        \"\"\"\n",
    "        归一化\n",
    "        :param values: 要归一化的值\n",
    "        :param shift_mean: 负一化方式\n",
    "        :return: 返回归一化之后的结果\n",
    "        \"\"\"\n",
    "        mean, var = torch.mean(values), torch.var(values)\n",
    "        whitened = (values - mean) * torch.rsqrt(var + 1e-8)\n",
    "        if not shift_mean:\n",
    "            whitened += mean\n",
    "        return whitened"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7100e25-b9c9-425a-8467-0c1318a676ee",
   "metadata": {},
   "source": [
    "# main prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55040c77-6e54-4bfa-8bff-afea2225438c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ab33b9-5ea4-4253-91e4-09c30c4a072e",
   "metadata": {},
   "source": [
    "## ActorCriticLoraModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f5bec2a-fcc3-4283-9a9c-ce7365effc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoraModel(PeftModel):\n",
    "    def __init__(self, config: Config, model):\n",
    "        lora_args = LoraArguments()\n",
    "        lora_config = LoraConfig(\n",
    "            r=lora_args.lora_r,\n",
    "            lora_alpha=lora_args.lora_alpha,\n",
    "            target_modules=lora_args.lora_target_modules,\n",
    "            lora_dropout=lora_args.lora_dropout,\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "        )\n",
    "        super().__init__(model, lora_config)\n",
    "        self.v_head = torch.nn.Linear(1024, 1, bias=False).to(config.device)\n",
    "        if lora_args.is_reload_trained_params:\n",
    "            super().from_pretrained(model, config.save_lora_path)\n",
    "            self.v_head.load_state_dict(torch.load(config.save_v_head_path))\n",
    "        for name, module in self.named_modules():\n",
    "            if 'lora_' in name:\n",
    "                for param in module.parameters():\n",
    "                    param.requires_grad = True\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, tools: Tools):\n",
    "        res = super().forward(input_ids, attention_mask, output_hidden_states=True)\n",
    "        values = self.v_head(res.hidden_states[0]).squeeze(-1)[:, :-1]\n",
    "        values = tools.filter_mask(values)\n",
    "        probs = tools.probs_from_logits(res.logits[:, :-1, :], input_ids[:, 1:])\n",
    "        probs = tools.filter_mask(probs)\n",
    "        return probs, values\n",
    "\n",
    "\n",
    "class ActorCriticLoraModel(torch.nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        model = AutoModelForCausalLM.from_pretrained(config.gpt_model).to(config.device).eval()\n",
    "        self.model = LoraModel(config, model)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.gpt_model)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, tools: Tools):\n",
    "        probs, values = self.model(input_ids, attention_mask, tools)\n",
    "        return probs, values\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def actor_generate(self, input_ids):\n",
    "        generated_ids = self.model.generate(input_ids, max_new_tokens=512, top_p=1.0,\n",
    "                                            num_beams=1,\n",
    "                                            do_sample=False)\n",
    "        response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        response_id = generated_ids[:, input_ids.shape[1]:]\n",
    "        return response, generated_ids, response_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469e9a09-ac86-4281-9c6c-545c9462143e",
   "metadata": {},
   "source": [
    "## ReferenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "231fcfdb-dd69-4925-8e62-5fc455d78b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cd6582-38de-4367-88fa-e261a0e3ac49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b19df0e-8043-4546-ad16-af8b971e9844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2SdpaAttention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "          (up_proj): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "          (down_proj): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm()\n",
       "        (post_attention_layernorm): Qwen2RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AutoModelForCausalLM.from_pretrained(Config.gpt_model).to(torch.device(\"mps\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9693d7b-651a-41f5-96bf-d40fd0a4e05f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b392ccf9-e2c6-4730-b109-a22a88c96b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReferenceModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(ReferenceModel, self).__init__()\n",
    "        self.config = config\n",
    "        self.reference_model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.config.gpt_model, \n",
    "            # torch_dtype=\"auto\"\n",
    "        ).to(\n",
    "            self.config.device\n",
    "        )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, input_ids, attention_mask, tools: Tools):\n",
    "        logits = self.reference_model(input_ids=input_ids,\n",
    "                                      attention_mask=attention_mask).logits\n",
    "        prob_refs = tools.probs_from_logits(logits[:, :-1, :], input_ids[:, 1:])\n",
    "        prob_refs = tools.filter_mask(prob_refs)\n",
    "        return prob_refs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3087d1cb-e186-4c59-87e7-e96088f5020b",
   "metadata": {},
   "source": [
    "## RewardModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449cde3c-263b-4ac6-ac11-6a9a4df54ee6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90d3847e-8e61-44c6-bd00-9fad22fcb6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "\n",
    "# 奖励模型\n",
    "class RewardModel(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super(RewardModel, self).__init__()\n",
    "        self.config = config\n",
    "        self.reward_tokenizer = BertTokenizer.from_pretrained(self.config.Sentiment_model)\n",
    "        self.reward_model = BertForSequenceClassification.from_pretrained(self.config.Sentiment_model).to(\n",
    "            self.config.device)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, text):\n",
    "        input_ids, attention_mask = self.data_process(text)\n",
    "        output = self.reward_model(torch.tensor(input_ids).to(self.config.device),\n",
    "                                   torch.tensor(attention_mask).to(self.config.device))\n",
    "        probs = torch.softmax(torch.tensor(output.logits), dim=1).tolist()\n",
    "        reward = [prob[0] for prob in probs]\n",
    "        return reward\n",
    "\n",
    "    def data_process(self, texts):\n",
    "        attention_mask = []\n",
    "        input_ids = [self.reward_tokenizer.encode(text)[:512] for text in texts]\n",
    "        max_length = max(len(i) for i in input_ids)\n",
    "        res = []\n",
    "        for one in input_ids:\n",
    "            padding_num = max_length - len(one)\n",
    "            res.append(one + [self.reward_tokenizer.pad_token_id] * padding_num)\n",
    "            attention_mask.append([1] * len(one) + [0] * padding_num)\n",
    "        return res, attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c99ce2-be4a-4988-bbd5-7a901865b63c",
   "metadata": {},
   "source": [
    "## CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "12d3e2b0-ff2c-4dc9-b0a4-66815d6b3a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "140051c0-c21c-47fb-bfab-c196046c3c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_file, actor_tokenizer):\n",
    "        self.actor_tokenizer = actor_tokenizer\n",
    "        with open(data_file, 'r', encoding=\"utf-8\") as f:\n",
    "            self.data = json.load(f)\n",
    "        self.total_samples = len(self.data)\n",
    "        self.actor_padding_id = actor_tokenizer.pad_token_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 根据索引加载数据\n",
    "        # 这里可以根据需要从文件中读取数据并进行预处理\n",
    "        line = self.data[idx]\n",
    "        query = line[\"query\"]\n",
    "        system_content = line[\"system_content\"]\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_content},\n",
    "            {\"role\": \"user\", \"content\": query},\n",
    "        ]\n",
    "        text = self.actor_tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        model_inputs = self.actor_tokenizer([text], return_tensors=\"pt\")\n",
    "\n",
    "        return [model_inputs.input_ids.tolist()[0], model_inputs.attention_mask.tolist()[0]]\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        max_length = max([len(i[0]) for i in batch])\n",
    "        input_ids = []\n",
    "        mask_attention = []\n",
    "        for one in batch:\n",
    "            padding_num = max_length - len(one[0])\n",
    "            input_ids.append([self.actor_padding_id] * padding_num + one[0])\n",
    "            mask_attention.append([0] * padding_num + one[1])\n",
    "        return torch.tensor(input_ids), torch.tensor(mask_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed5048f-8478-498f-a0d4-d29fb49ff35c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d84e137-0b02-4bbc-aca0-77b8ddb435c4",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37a91ea6-b0f1-4151-a32a-9178344e950a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Tools:\n",
    "    def __init__(self, response_shape, response_ids_mask):\n",
    "        \"\"\"\n",
    "        :param response_shape: 模型生成的句子长度\n",
    "        :param response_ids_mask: 模型批量生成的句子中有padding，这里去除padding数据\n",
    "        \"\"\"\n",
    "        self.response_shape = response_shape\n",
    "        self.response_ids_mask = response_ids_mask\n",
    "\n",
    "    def filter_mask(self, values):\n",
    "        \"\"\"\n",
    "        :param values: 一般是prob_old、prob_ref、value(价值)的值\n",
    "        :return: 去除padding之后的数据\n",
    "        \"\"\"\n",
    "        return [value[-self.response_shape:][one_response_ids_mask] for value, one_response_ids_mask in\n",
    "                zip(values, self.response_ids_mask)]\n",
    "\n",
    "    @staticmethod\n",
    "    def probs_from_logits(logits, labels):\n",
    "        log_probs = F.log_softmax(logits, dim=2)\n",
    "        probs = torch.gather(log_probs, 2, labels.unsqueeze(2)).squeeze(-1)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0bbe57-b527-4fd1-a5a7-2242e4e1c1d0",
   "metadata": {},
   "source": [
    "# actual train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "43d118b4-8dcd-4679-af5a-6c3d1167fcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainPpo:\n",
    "    def __init__(self):\n",
    "        self.config = Config()\n",
    "        # 演员和评论家模型\n",
    "        self.actor_critic_model = ActorCriticLoraModel(self.config)\n",
    "        self.tokenizer = self.actor_critic_model.tokenizer\n",
    "        # 获得演员和评论家模型优化器, 这里使用的是lora, 不优化全量数据\n",
    "        self.actor_critic_opt = Adam(self.actor_critic_model.parameters(), lr=self.config.lr)\n",
    "        # 参考模型\n",
    "        self.reference_model = ReferenceModel(self.config)\n",
    "        # 奖励模型\n",
    "        self.reward_model = RewardModel(self.config)\n",
    "        # 训练数据\n",
    "        dataset = CustomDataset(self.config.data_path, self.tokenizer)\n",
    "        self.data_loader = DataLoader(dataset, batch_size=self.config.batch_size, shuffle=True,\n",
    "                                      collate_fn=dataset.collate_fn)\n",
    "        self.ppo = PPO(self.actor_critic_model, self.config, self.actor_critic_opt)\n",
    "\n",
    "    def train_ppo(self):\n",
    "        self.save_model()\n",
    "        for epoch in range(self.config.epochs):\n",
    "            print(epoch)\n",
    "            for batch_data in self.data_loader:\n",
    "                # 获得演员模型生成的结果(prompt_generate)和ids(prompt_generate_ids, generate_ids)\n",
    "                prompt_generate, prompt_generate_ids, generate_ids = self.actor_critic_model.actor_generate(\n",
    "                    batch_data[0].to(self.config.device)\n",
    "                )\n",
    "                attention_mask = (prompt_generate_ids != self.tokenizer.pad_token_id)\n",
    "                generate_ids_mask = (generate_ids[:, :-1] != self.tokenizer.pad_token_id)\n",
    "                # 模型生成的token, 为什么减去1，因为最后一个字符是结束符\n",
    "                response_shape = generate_ids.shape[1] - 1\n",
    "                # 初始化工具\n",
    "                tools = Tools(response_shape, generate_ids_mask)\n",
    "                # 去掉输入，获得真正生成的数据。用于计算reword value\n",
    "                pure_generate = [one.split(\"assistant\\n\")[1] for one in prompt_generate]\n",
    "                reward = self.reward_model(pure_generate)\n",
    "                # 获得参考模型probs\n",
    "                prob_refs = self.reference_model(prompt_generate_ids, attention_mask, tools)\n",
    "                # 获得上帝模型（评论家模型）的价值\n",
    "                self.ppo.train(prompt_generate_ids, attention_mask, prob_refs, reward, tools)\n",
    "        self.save_model()\n",
    "\n",
    "    def save_model(self):\n",
    "        # 保存lora参数\n",
    "        self.actor_critic_model.model.save_pretrained(self.config.save_lora_path, safe_serialization=False)\n",
    "        # 保存价值模型参数\n",
    "        torch.save(self.actor_critic_model.model.v_head.state_dict(), self.config.save_v_head_path)\n",
    "\n",
    "\n",
    "\n",
    "train_ppo = TrainPpo()\n",
    "train_ppo.train_ppo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b163987c-6eda-4170-b770-3bc7eda5ece9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9e0b6a-a1f7-4c77-a277-03bf96f9ad17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca66d57d-4890-46ae-bc50-03cda95feccc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7cf0f49c-e28d-4ef2-a50b-7b6632fada1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "The operator 'aten::isin.Tensor_Tensor_out' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m train_ppo \u001b[38;5;241m=\u001b[39m TrainPpo()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain_ppo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_ppo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[34], line 25\u001b[0m, in \u001b[0;36mTrainPpo.train_ppo\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(epoch)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_loader:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# 获得演员模型生成的结果(prompt_generate)和ids(prompt_generate_ids, generate_ids)\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m     prompt_generate, prompt_generate_ids, generate_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor_critic_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m (prompt_generate_ids \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpad_token_id)\n\u001b[1;32m     29\u001b[0m     generate_ids_mask \u001b[38;5;241m=\u001b[39m (generate_ids[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpad_token_id)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ml12/lib/python3.12/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 43\u001b[0m, in \u001b[0;36mActorCriticLoraModel.actor_generate\u001b[0;34m(self, input_ids)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mactor_generate\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids):\n\u001b[0;32m---> 43\u001b[0m     generated_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(generated_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     47\u001b[0m     response_id \u001b[38;5;241m=\u001b[39m generated_ids[:, input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ml12/lib/python3.12/site-packages/peft/peft_model.py:647\u001b[0m, in \u001b[0;36mPeftModel.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    646\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m--> 647\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_base_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ml12/lib/python3.12/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ml12/lib/python3.12/site-packages/transformers/generation/utils.py:1576\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1559\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assisted_decoding(\n\u001b[1;32m   1560\u001b[0m         input_ids,\n\u001b[1;32m   1561\u001b[0m         candidate_generator\u001b[38;5;241m=\u001b[39mcandidate_generator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1572\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1573\u001b[0m     )\n\u001b[1;32m   1574\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1575\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1576\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_greedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1577\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1581\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1582\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1584\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1586\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1587\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1589\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1590\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ml12/lib/python3.12/site-packages/transformers/generation/utils.py:2548\u001b[0m, in \u001b[0;36mGenerationMixin._greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2541\u001b[0m         streamer\u001b[38;5;241m.\u001b[39mput(next_tokens\u001b[38;5;241m.\u001b[39mcpu())\n\u001b[1;32m   2542\u001b[0m     model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   2543\u001b[0m         outputs,\n\u001b[1;32m   2544\u001b[0m         model_kwargs,\n\u001b[1;32m   2545\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2546\u001b[0m     )\n\u001b[0;32m-> 2548\u001b[0m     unfinished_sequences \u001b[38;5;241m=\u001b[39m unfinished_sequences \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m~\u001b[39m\u001b[43mstopping_criteria\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2549\u001b[0m     this_peer_finished \u001b[38;5;241m=\u001b[39m unfinished_sequences\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   2551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streamer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ml12/lib/python3.12/site-packages/transformers/generation/stopping_criteria.py:158\u001b[0m, in \u001b[0;36mStoppingCriteriaList.__call__\u001b[0;34m(self, input_ids, scores, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m is_done \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull((input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],), \u001b[38;5;28;01mFalse\u001b[39;00m, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m criteria \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 158\u001b[0m     is_done \u001b[38;5;241m=\u001b[39m is_done \u001b[38;5;241m|\u001b[39m \u001b[43mcriteria\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m is_done\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ml12/lib/python3.12/site-packages/transformers/generation/stopping_criteria.py:149\u001b[0m, in \u001b[0;36mEosTokenCriteria.__call__\u001b[0;34m(self, input_ids, scores, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;129m@add_start_docstrings\u001b[39m(STOPPING_CRITERIA_INPUTS_DOCSTRING)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids: torch\u001b[38;5;241m.\u001b[39mLongTensor, scores: torch\u001b[38;5;241m.\u001b[39mFloatTensor, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mBoolTensor:\n\u001b[0;32m--> 149\u001b[0m     is_done \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misin\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m is_done\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: The operator 'aten::isin.Tensor_Tensor_out' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5129672-a33f-4760-b7d7-b4ca48df485c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87034985-6f3c-4181-93d2-3e23ad38e1e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393e1c0f-b1dd-455a-bc49-2c111c3d0601",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99deb56-c61d-46c1-9390-b97aa20f830f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87562523-743d-445b-92bc-5098641b6b40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8671506e-a4da-4241-a3fd-c63531e87ad9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d0e419-eef4-4268-8bfc-cc6df71a4157",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d4a51c-116e-4aa7-a711-aff260b3a47c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ddf5c8-3bd7-4eb7-a58b-2e495e8f1770",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527a659c-4493-4ad8-9984-55b15046da71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0c2194-1488-41f7-93fc-0b9e3ac834da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c9278c-2e67-4caa-ad97-e8d87c7f2f97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
