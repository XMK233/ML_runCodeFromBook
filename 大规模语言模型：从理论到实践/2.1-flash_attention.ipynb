{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd48d50d-0027-498e-b327-902d1a617055",
   "metadata": {},
   "outputs": [],
   "source": [
    "## flash attention是一个算法。\n",
    "## 大概的意思就是，能够不要读取一整个QKV矩阵，还能够开展attention计算。\n",
    "## 我擦嘞。\n",
    "\n",
    "## 额外参考了：\n",
    "### https://zhuanlan.zhihu.com/p/676655352"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1aaec59a-8912-46fb-8265-164e0af46665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "30411dea-0d74-4cb4-b113-7e5eb5f4474a",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 80 \n",
    "d = 32\n",
    "M = 1000 ## SRAM 大小为M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c84359be-248e-4958-a993-2ffad5bb331b",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_linear = nn.Linear(d, d)\n",
    "k_linear = nn.Linear(d, d)\n",
    "v_linear = nn.Linear(d, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7d4a9273-1c09-483d-8434-501f9ba59c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = nn.Embedding(N, d).weight ## 这个就是咱们姑且这么设定吧。\n",
    "q = q_linear(x)\n",
    "k = k_linear(x)\n",
    "v = v_linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "dac1d1c9-faa0-477d-86f2-ce2d38f0d157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 8)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bc = math.ceil(\n",
    "    M/(4*d)\n",
    ") ## 设置块大小，每块大小为这么大。。。\n",
    "Br = min(\n",
    "    math.ceil(\n",
    "        M/(4*d)\n",
    "    ),\n",
    "    d\n",
    ") ## 设置块大小，每块大小为这么大。。。\n",
    "Bc, Br"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "11673e4d-8cac-40f1-95a8-7d8f7a3ca9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 在HBM中初始化：\n",
    "o = torch.zeros(N, d)\n",
    "l = torch.zeros(N)\n",
    "m = torch.tensor([float('-inf')] * N, dtype=torch.float32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5680ff20-5d1a-4066-bbdf-1bddc6c65b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tr = math.ceil(N/Br)\n",
    "Tc = math.ceil(N/Bc)\n",
    "Tr, Tc ## 将矩阵分成这么多块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "938a2a07-01a3-4254-b513-c3eba678d0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_blocks = torch.split(q, Br, dim=0) ## 这个是新学到的方法，也就是将原来的矩阵分块。\n",
    "o_blocks = torch.split(o, Br, dim=0) ## 这个是新学到的方法，也就是将原来的矩阵分块。\n",
    "k_blocks = torch.split(k, Bc, dim=0) ## 这个是新学到的方法，也就是将原来的矩阵分块。\n",
    "v_blocks = torch.split(v, Bc, dim=0) ## 这个是新学到的方法，也就是将原来的矩阵分块。\n",
    "\n",
    "l_blocks = torch.split(l, Br, dim=0)\n",
    "m_blocks = torch.split(m, Br, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "45bc2ca4-0fb4-4c21-9427-97762e61c012",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_o_blocks, new_l_blocks, new_m_blocks = [None] * len(o_blocks), [None] * len(l_blocks), [None] * len(m_blocks)\n",
    "\n",
    "for j in range(Tc):\n",
    "    Kj, Vj = k_blocks[j], v_blocks[j]\n",
    "    for i in range(Tr):\n",
    "        mi = m_blocks[i]\n",
    "        li = l_blocks[i]\n",
    "        Oi = o_blocks[i]\n",
    "        \n",
    "        Qi = q_blocks[i]\n",
    "        Sij = torch.einsum('...id,...jd->...ij', Qi, Kj)\n",
    "        mij, _ = Sij.max(axis=1, keepdims=True)\n",
    "        Pij = torch.exp(Sij - mij)\n",
    "        lij = Pij.sum(axis=1, keepdims=True)\n",
    "\n",
    "        mi_new = torch.maximum(mi.unsqueeze(1), mij)\n",
    "        li_new = torch.exp(mi.unsqueeze(1) - mi_new) * li.unsqueeze(1) + torch.exp(mij - mi_new) * lij\n",
    "\n",
    "        new_o_blocks[i] = \\\n",
    "        (li.unsqueeze(1) / li_new) * torch.exp(mi.unsqueeze(1) - mi_new) * Oi\\\n",
    "        + (torch.exp(mij - mi_new) / li_new) * torch.mm(\n",
    "            Pij, Vj\n",
    "        )\n",
    "        new_l_blocks[i] = li_new\n",
    "        new_m_blocks[i] = mi_new\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "3f950c18-3da3-409b-ab1e-d8aa24305108",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = torch.cat(new_o_blocks, dim=0)\n",
    "l = torch.cat(new_l_blocks, dim=0)\n",
    "m = torch.cat(new_m_blocks, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "399f90d3-c9c9-496b-9029-b92c70867f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 总结了一下：\n",
    "### 对角矩阵的逆矩阵，实际上就是对角元素变成原来的倒数（0的话还是变成0）\n",
    "### 算法有的时候没有实现对。\n",
    "### 书本上的算法本身有的时候就是错的。。。这个感觉防不胜防。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1554e35-17c9-4a0d-941a-50caeec042a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
